{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19327bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import required modules from the project\n",
    "import sys\n",
    "import classifiers\n",
    "import transformers\n",
    "import train_test\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_cached_pipeline(n_genes, pipelines_dir):\n",
    "    \"\"\"\n",
    "    Load a cached pipeline for the given n_genes value.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_genes : int\n",
    "        Number of genes for feature selection\n",
    "    pipelines_dir : str\n",
    "        Path to the pipelines cache directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        Fitted preprocessing pipeline\n",
    "    \"\"\"\n",
    "    pipeline_filename = f\"pipeline_ngenes_{n_genes}.pkl\"\n",
    "    pipeline_path = os.path.join(pipelines_dir, pipeline_filename)\n",
    "    \n",
    "    if os.path.exists(pipeline_path):\n",
    "        print(f\"  Loading cached pipeline for n_genes={n_genes}: {pipeline_path}\")\n",
    "        return joblib.load(pipeline_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Cached pipeline not found: {pipeline_path}\")\n",
    "\n",
    "\n",
    "def load_training_gene_order():\n",
    "    \"\"\"\n",
    "    Load the original training data gene order from counts file.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    training_genes : list\n",
    "        List of gene names in the same order as training data\n",
    "    \"\"\"\n",
    "    \n",
    "    counts_file = \"../data/counts_20aug25.csv\"\n",
    "    \n",
    "    print(f\"Loading training gene order from {counts_file}\")\n",
    "    \n",
    "    # Read just the first row to get gene names (column headers)\n",
    "    # The training data has genes as rows, so we need the index\n",
    "    df_header = pd.read_csv(counts_file, nrows=1)\n",
    "    \n",
    "    # Get the first column name (should be gene identifier column)\n",
    "    gene_col = df_header.columns[0]\n",
    "    \n",
    "    # Now read just the gene column to get all gene names\n",
    "    df_genes = pd.read_csv(counts_file, usecols=[gene_col])\n",
    "    training_genes = df_genes[gene_col].tolist()\n",
    "    \n",
    "    print(f\"Found {len(training_genes)} genes in training data\")\n",
    "    \n",
    "    return training_genes\n",
    "\n",
    "\n",
    "def load_new_samples(input_file):\n",
    "    \"\"\"\n",
    "    Load new samples from CSV file and reorder genes to match training data.\n",
    "    Expected format: samples on rows, genes (ENS...) on columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the CSV file containing new samples\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.ndarray\n",
    "        Gene expression data (samples x genes) in training gene order\n",
    "    sample_names : list\n",
    "        Sample identifiers from row names\n",
    "    \"\"\"\n",
    "    print(f\"Loading new samples from {input_file}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_file, index_col=0)\n",
    "    \n",
    "    # Extract sample names from index\n",
    "    sample_names = df.index.tolist()\n",
    "    \n",
    "    # Get training gene order\n",
    "    training_genes = load_training_gene_order()\n",
    "    \n",
    "    # Check which genes are available in new data\n",
    "    available_genes = set(df.columns)\n",
    "    training_genes_set = set(training_genes)\n",
    "    \n",
    "    missing_genes = training_genes_set - available_genes\n",
    "    extra_genes = available_genes - training_genes_set\n",
    "    \n",
    "    print(f\"Loaded {df.shape[0]} samples with {df.shape[1]} genes\")\n",
    "    print(f\"Training data expects {len(training_genes)} genes\")\n",
    "    print(f\"Missing genes: {len(missing_genes)}\")\n",
    "    print(f\"Extra genes: {len(extra_genes)}\")\n",
    "    \n",
    "    if missing_genes:\n",
    "        print(f\"WARNING: {len(missing_genes)} genes from training data are missing in new data\")\n",
    "        if len(missing_genes) <= 10:\n",
    "            print(f\"Missing genes: {list(missing_genes)[:10]}\")\n",
    "        else:\n",
    "            print(f\"First 10 missing genes: {list(missing_genes)[:10]}\")\n",
    "    \n",
    "    # Reorder columns to match training data and fill missing genes with zeros\n",
    "    X_reordered = np.zeros((df.shape[0], len(training_genes)), dtype=np.float32)\n",
    "    \n",
    "    for i, gene in enumerate(training_genes):\n",
    "        if gene in df.columns:\n",
    "            X_reordered[:, i] = df[gene].values.astype(np.float32)\n",
    "        else:\n",
    "            # Missing gene - fill with zeros (or could use median/mean)\n",
    "            X_reordered[:, i] = 0.0\n",
    "    \n",
    "    print(f\"Reordered data shape: {X_reordered.shape}\")\n",
    "    \n",
    "    return X_reordered, sample_names\n",
    "\n",
    "\n",
    "def load_models_and_metadata(models_dir, pipelines_dir=None):\n",
    "    \"\"\"\n",
    "    Load all final models and their metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dir : str\n",
    "        Path to the final_models directory\n",
    "    pipelines_dir : str, optional\n",
    "        Path to the pipelines cache directory. If provided, will load cached pipelines\n",
    "        based on n_genes from model metadata. Otherwise, loads reference pipelines.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    models : dict\n",
    "        Dictionary containing loaded models and metadata for each model type\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Global pipeline cache to avoid loading the same pipeline multiple times\n",
    "    global_pipeline_cache = {}\n",
    "    \n",
    "    # Load NN model (standard multiclass)\n",
    "    nn_dir = os.path.join(models_dir, \"NN\")\n",
    "    if os.path.exists(nn_dir):\n",
    "        print(\"Loading NN model...\")\n",
    "        \n",
    "        # Load model\n",
    "        model_path = os.path.join(nn_dir, \"NN_final_CV_standard_model_0.pkl\")\n",
    "        with open(model_path, 'rb') as f:\n",
    "            nn_model = joblib.load(f)\n",
    "        \n",
    "        # Load label mapping\n",
    "        label_mapping_path = os.path.join(nn_dir, \"label_mapping_NN_CV_standard.json\")\n",
    "        with open(label_mapping_path, 'r') as f:\n",
    "            nn_label_mapping = json.load(f)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(nn_dir, \"NN_final_CV_standard_model_0_metadata.json\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            nn_metadata = json.load(f)\n",
    "        \n",
    "        # Load pipeline based on n_genes from metadata\n",
    "        if pipelines_dir is not None and 'model_info' in nn_metadata and 'n_genes' in nn_metadata['model_info']:\n",
    "            n_genes = nn_metadata['model_info']['n_genes']\n",
    "            if n_genes not in global_pipeline_cache:\n",
    "                global_pipeline_cache[n_genes] = load_cached_pipeline(n_genes, pipelines_dir)\n",
    "            nn_pipeline = global_pipeline_cache[n_genes]\n",
    "        else:\n",
    "            # Fallback to reference pipeline\n",
    "            pipeline_path = os.path.join(nn_dir, \"pipeline_NN_CV_standard.pkl\")\n",
    "            with open(pipeline_path, 'rb') as f:\n",
    "                nn_pipeline = joblib.load(f)\n",
    "        \n",
    "        models['NN'] = {\n",
    "            'model': nn_model,\n",
    "            'pipeline': nn_pipeline,\n",
    "            'label_mapping': nn_label_mapping,\n",
    "            'metadata': nn_metadata,\n",
    "            'multi_type': 'standard'\n",
    "        }\n",
    "    \n",
    "    # Load SVM models (OvR multiclass)\n",
    "    svm_dir = os.path.join(models_dir, \"SVM\")\n",
    "    if os.path.exists(svm_dir):\n",
    "        print(\"Loading SVM models...\")\n",
    "        \n",
    "        # Load label mapping\n",
    "        label_mapping_path = os.path.join(svm_dir, \"label_mapping_SVM_CV_OvR.json\")\n",
    "        with open(label_mapping_path, 'r') as f:\n",
    "            svm_label_mapping = json.load(f)\n",
    "        \n",
    "        # Load all class-specific models and their pipelines\n",
    "        svm_models = {}\n",
    "        svm_metadata = {}\n",
    "        svm_pipelines = {}  # Store pipeline for each class\n",
    "        \n",
    "        for file in os.listdir(svm_dir):\n",
    "            if file.endswith('.pkl') and 'class_' in file:\n",
    "                # Extract class name from filename\n",
    "                class_name = file.replace('SVM_final_CV_OvR_class_', '').replace('.pkl', '')\n",
    "                class_name = class_name.split('_model_')[0]\n",
    "                \n",
    "                # Load model\n",
    "                model_path = os.path.join(svm_dir, file)\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    svm_models[class_name] = joblib.load(f)\n",
    "                \n",
    "                # Load corresponding metadata\n",
    "                metadata_file = file.replace('.pkl', '_metadata.json')\n",
    "                metadata_path = os.path.join(svm_dir, metadata_file)\n",
    "                if os.path.exists(metadata_path):\n",
    "                    with open(metadata_path, 'r') as f:\n",
    "                        svm_metadata[class_name] = json.load(f)\n",
    "                    \n",
    "                    # Load pipeline based on n_genes from this class's metadata\n",
    "                    if pipelines_dir is not None and 'model_info' in svm_metadata[class_name] and 'n_genes' in svm_metadata[class_name]['model_info']:\n",
    "                        n_genes = svm_metadata[class_name]['model_info']['n_genes']\n",
    "                        if n_genes not in global_pipeline_cache:\n",
    "                            global_pipeline_cache[n_genes] = load_cached_pipeline(n_genes, pipelines_dir)\n",
    "                        svm_pipelines[class_name] = global_pipeline_cache[n_genes]\n",
    "        \n",
    "        # Fallback to reference pipeline if no cached pipelines loaded\n",
    "        if not svm_pipelines and pipelines_dir is None:\n",
    "            pipeline_path = os.path.join(svm_dir, \"pipeline_SVM_CV_OvR.pkl\")\n",
    "            if os.path.exists(pipeline_path):\n",
    "                with open(pipeline_path, 'rb') as f:\n",
    "                    reference_pipeline = joblib.load(f)\n",
    "                # Use the same pipeline for all classes as fallback\n",
    "                for class_name in svm_models.keys():\n",
    "                    svm_pipelines[class_name] = reference_pipeline\n",
    "        \n",
    "        models['SVM'] = {\n",
    "            'models': svm_models,\n",
    "            'pipelines': svm_pipelines,  # Changed from single pipeline to per-class pipelines\n",
    "            'label_mapping': svm_label_mapping,\n",
    "            'metadata': svm_metadata,\n",
    "            'multi_type': 'ovr'\n",
    "        }\n",
    "    \n",
    "    # Load XGBOOST models (OvR multiclass)\n",
    "    xgb_dir = os.path.join(models_dir, \"XGBOOST\")\n",
    "    if os.path.exists(xgb_dir):\n",
    "        print(\"Loading XGBOOST models...\")\n",
    "        \n",
    "        # Load label mapping\n",
    "        label_mapping_path = os.path.join(xgb_dir, \"label_mapping_XGBOOST_CV_OvR.json\")\n",
    "        with open(label_mapping_path, 'r') as f:\n",
    "            xgb_label_mapping = json.load(f)\n",
    "        \n",
    "        # Load all class-specific models and their pipelines\n",
    "        xgb_models = {}\n",
    "        xgb_metadata = {}\n",
    "        xgb_pipelines = {}  # Store pipeline for each class\n",
    "        \n",
    "        for file in os.listdir(xgb_dir):\n",
    "            if file.endswith('.pkl') and 'class_' in file:\n",
    "                # Extract class name from filename\n",
    "                class_name = file.replace('XGBOOST_final_CV_OvR_class_', '').replace('.pkl', '')\n",
    "                class_name = class_name.split('_model_')[0]\n",
    "                \n",
    "                # Load model\n",
    "                model_path = os.path.join(xgb_dir, file)\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    xgb_models[class_name] = joblib.load(f)\n",
    "                \n",
    "                # Load corresponding metadata\n",
    "                metadata_file = file.replace('.pkl', '_metadata.json')\n",
    "                metadata_path = os.path.join(xgb_dir, metadata_file)\n",
    "                if os.path.exists(metadata_path):\n",
    "                    with open(metadata_path, 'r') as f:\n",
    "                        xgb_metadata[class_name] = json.load(f)\n",
    "                    \n",
    "                    # Load pipeline based on n_genes from this class's metadata\n",
    "                    if pipelines_dir is not None and 'model_info' in xgb_metadata[class_name] and 'n_genes' in xgb_metadata[class_name]['model_info']:\n",
    "                        n_genes = xgb_metadata[class_name]['model_info']['n_genes']\n",
    "                        if n_genes not in global_pipeline_cache:\n",
    "                            global_pipeline_cache[n_genes] = load_cached_pipeline(n_genes, pipelines_dir)\n",
    "                        xgb_pipelines[class_name] = global_pipeline_cache[n_genes]\n",
    "        \n",
    "        # Fallback to reference pipeline if no cached pipelines loaded\n",
    "        if not xgb_pipelines and pipelines_dir is None:\n",
    "            pipeline_path = os.path.join(xgb_dir, \"pipeline_XGBOOST_CV_OvR.pkl\")\n",
    "            if os.path.exists(pipeline_path):\n",
    "                with open(pipeline_path, 'rb') as f:\n",
    "                    reference_pipeline = joblib.load(f)\n",
    "                # Use the same pipeline for all classes as fallback\n",
    "                for class_name in xgb_models.keys():\n",
    "                    xgb_pipelines[class_name] = reference_pipeline\n",
    "        \n",
    "        models['XGBOOST'] = {\n",
    "            'models': xgb_models,\n",
    "            'pipelines': xgb_pipelines,  # Changed from single pipeline to per-class pipelines\n",
    "            'label_mapping': xgb_label_mapping,\n",
    "            'metadata': xgb_metadata,\n",
    "            'multi_type': 'ovr'\n",
    "        }\n",
    "    \n",
    "    # Print summary of loaded pipelines\n",
    "    if global_pipeline_cache:\n",
    "        print(f\"\\nLoaded {len(global_pipeline_cache)} unique pipelines for n_genes: {sorted(global_pipeline_cache.keys())}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def load_ensemble_weights(weights_dir):\n",
    "    \"\"\"\n",
    "    Load ensemble weights for both global and OvR ensemble methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    weights_dir : str\n",
    "        Path to the ensemble weights directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ensemble_weights : dict\n",
    "        Dictionary containing ensemble weights\n",
    "    \"\"\"\n",
    "    ensemble_weights = {}\n",
    "    \n",
    "    # Load global ensemble weights\n",
    "    global_weights_path = os.path.join(weights_dir, \"cv\", \"global_ensemble_weights_used.csv\")\n",
    "    if os.path.exists(global_weights_path):\n",
    "        global_weights = pd.read_csv(global_weights_path)\n",
    "        ensemble_weights['global'] = global_weights\n",
    "        print(\"Loaded global ensemble weights\")\n",
    "    \n",
    "    # Load OvR ensemble weights\n",
    "    ovr_weights_path = os.path.join(weights_dir, \"cv\", \"ovr_ensemble_weights_used.csv\")\n",
    "    if os.path.exists(ovr_weights_path):\n",
    "        ovr_weights = pd.read_csv(ovr_weights_path)\n",
    "        ensemble_weights['ovr'] = ovr_weights\n",
    "        print(\"Loaded OvR ensemble weights\")\n",
    "    \n",
    "    return ensemble_weights\n",
    "\n",
    "\n",
    "def load_cutoffs(cutoffs_path):\n",
    "    \"\"\"\n",
    "    Load prediction cutoffs for CV source.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cutoffs_path : str\n",
    "        Path to the cutoffs CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cutoffs : dict\n",
    "        Dictionary containing cutoffs for each model\n",
    "    \"\"\"\n",
    "    cutoffs_df = pd.read_csv(cutoffs_path)\n",
    "    \n",
    "    # Filter for CV source only\n",
    "    cv_cutoffs = cutoffs_df[cutoffs_df['source'] == 'cv'].copy()\n",
    "    \n",
    "    cutoffs = {}\n",
    "    for _, row in cv_cutoffs.iterrows():\n",
    "        cutoffs[row['model']] = row['prob_cutoff']\n",
    "    \n",
    "    print(f\"Loaded cutoffs for {len(cutoffs)} models\")\n",
    "    return cutoffs\n",
    "\n",
    "\n",
    "def predict_nn_standard(X, models, sample_names):\n",
    "    \"\"\"\n",
    "    Make predictions using the NN model (standard multiclass).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Input data (samples x genes)\n",
    "    models : dict\n",
    "        Dictionary containing NN model info\n",
    "    sample_names : list\n",
    "        Sample identifiers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with predictions, probabilities, and sample info\n",
    "    \"\"\"\n",
    "    print(\"Making NN predictions...\")\n",
    "    \n",
    "    nn_info = models['NN']\n",
    "    pipeline = nn_info['pipeline']\n",
    "    model = nn_info['model']\n",
    "    label_mapping = nn_info['label_mapping']\n",
    "    \n",
    "    # Create reverse mapping (encoded -> original labels)\n",
    "    reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    # Preprocess data using the pipeline\n",
    "    # Note: We need to provide dummy study labels for preprocessing\n",
    "    dummy_studies = np.zeros(X.shape[0])  # Assuming all samples from same study\n",
    "    X_processed = pipeline.transform(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_probs = model.predict_proba(X_processed)\n",
    "    pred_classes = np.argmax(pred_probs, axis=1)\n",
    "    \n",
    "    # Convert back to original labels\n",
    "    pred_labels = [reverse_mapping[cls] for cls in pred_classes]\n",
    "    \n",
    "    # Get maximum probability for each prediction\n",
    "    max_probs = np.max(pred_probs, axis=1)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'sample_name': sample_names,\n",
    "        'sample_index': range(len(sample_names)),\n",
    "        'prediction': pred_labels,\n",
    "        'prediction_prob': max_probs,\n",
    "        'prediction_passed_cutoff': False  # Will be filled later with cutoffs\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def predict_single_class(class_name, model, X_processed):\n",
    "    \"\"\"\n",
    "    Make predictions for a single class model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    class_name : str\n",
    "        Name of the class\n",
    "    model : sklearn model\n",
    "        Trained model for this class\n",
    "    X_processed : np.ndarray\n",
    "        Preprocessed input data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (class_name, probabilities, predictions)\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba(X_processed)\n",
    "        if probs.shape[1] == 2:  # Binary classification\n",
    "            class_probs = probs[:, 1]  # Probability of positive class\n",
    "            class_preds = (probs[:, 1] >= 0.5).astype(int)\n",
    "        else:\n",
    "            class_probs = np.max(probs, axis=1)\n",
    "            class_preds = np.argmax(probs, axis=1)\n",
    "    else:\n",
    "        # For models that only support decision_function\n",
    "        scores = model.decision_function(X_processed)\n",
    "        class_probs = 1 / (1 + np.exp(-scores))\n",
    "        class_preds = (scores >= 0).astype(int)\n",
    "    \n",
    "    return class_name, class_probs, class_preds\n",
    "\n",
    "\n",
    "def predict_ovr_models(X, models, model_type, sample_names):\n",
    "    \"\"\"\n",
    "    Make predictions using OvR models (SVM or XGBOOST).\n",
    "    Each class may use a different pipeline based on its n_genes hyperparameter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Input data (samples x genes)\n",
    "    models : dict\n",
    "        Dictionary containing model info\n",
    "    model_type : str\n",
    "        'SVM' or 'XGBOOST'\n",
    "    sample_names : list\n",
    "        Sample identifiers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with predictions, probabilities, and sample info\n",
    "    \"\"\"\n",
    "    print(f\"Making {model_type} predictions...\")\n",
    "    \n",
    "    model_info = models[model_type]\n",
    "    class_models = model_info['models']\n",
    "    class_pipelines = model_info.get('pipelines', {})\n",
    "    label_mapping = model_info['label_mapping']\n",
    "    \n",
    "    # Group classes by pipeline to minimize data processing\n",
    "    pipeline_groups = {}\n",
    "    for class_name, model in class_models.items():\n",
    "        if class_name in class_pipelines:\n",
    "            pipeline = class_pipelines[class_name]\n",
    "        else:\n",
    "            pipeline = next(iter(class_pipelines.values())) if class_pipelines else None\n",
    "            if pipeline is None:\n",
    "                raise ValueError(f\"No pipeline available for class {class_name}\")\n",
    "        \n",
    "        pipeline_id = id(pipeline)\n",
    "        if pipeline_id not in pipeline_groups:\n",
    "            pipeline_groups[pipeline_id] = {'pipeline': pipeline, 'classes': []}\n",
    "        pipeline_groups[pipeline_id]['classes'].append((class_name, model))\n",
    "    \n",
    "    # Process data once per unique pipeline and make predictions for all classes using that pipeline\n",
    "    class_probabilities = {}\n",
    "    class_predictions = {}\n",
    "    \n",
    "    for pipeline_id, group_info in pipeline_groups.items():\n",
    "        pipeline = group_info['pipeline']\n",
    "        classes_with_models = group_info['classes']\n",
    "        \n",
    "        # Process data once for this pipeline\n",
    "        dummy_studies = np.zeros(X.shape[0])  # Assuming all samples from same study\n",
    "        X_processed = pipeline.transform(X)\n",
    "        print(f\"  Processed data for pipeline {pipeline_id} (used by {len(classes_with_models)} classes)\")\n",
    "        \n",
    "        # Make predictions for all classes using this processed data\n",
    "        for class_name, model in classes_with_models:\n",
    "            class_name, class_probs, class_preds = predict_single_class(class_name, model, X_processed)\n",
    "            class_probabilities[class_name] = class_probs\n",
    "            class_predictions[class_name] = class_preds\n",
    "    \n",
    "    # Use vectorized operations for faster aggregation\n",
    "    class_names = list(class_probabilities.keys())\n",
    "    prob_matrix = np.column_stack([class_probabilities[class_name] for class_name in class_names])\n",
    "    \n",
    "    # Find the class with highest probability for each sample using numpy (faster than pandas)\n",
    "    max_prob_indices = np.argmax(prob_matrix, axis=1)\n",
    "    max_probs = np.max(prob_matrix, axis=1)\n",
    "    pred_classes = [class_names[idx] for idx in max_prob_indices]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'sample_name': sample_names,\n",
    "        'sample_index': range(len(sample_names)),\n",
    "        'prediction': pred_classes,\n",
    "        'prediction_prob': max_probs,\n",
    "        'prediction_passed_cutoff': False  # Will be filled later with cutoffs\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def predict_ensemble_global(individual_predictions, ensemble_weights, sample_names):\n",
    "    \"\"\"\n",
    "    Make predictions using global ensemble method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    individual_predictions : dict\n",
    "        Dictionary containing pre-computed individual model predictions\n",
    "    ensemble_weights : dict\n",
    "        Dictionary containing ensemble weights\n",
    "    sample_names : list\n",
    "        Sample identifiers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with ensemble predictions\n",
    "    \"\"\"\n",
    "    print(\"Making Global Ensemble predictions...\")\n",
    "    \n",
    "    weights = ensemble_weights['global'].iloc[0]  # Should be only one row\n",
    "    \n",
    "    # Combine predictions using weights\n",
    "    ensemble_probs = np.zeros(len(sample_names))\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    for i in range(len(sample_names)):\n",
    "        weighted_votes = {}\n",
    "        \n",
    "        # Collect weighted votes from each model\n",
    "        for model_name, results in individual_predictions.items():\n",
    "            if results is None:\n",
    "                continue\n",
    "                \n",
    "            pred_class = results.iloc[i]['prediction']\n",
    "            pred_prob = results.iloc[i]['prediction_prob']\n",
    "            \n",
    "            if model_name == 'NN' and weights['nn_weight'] > 0:\n",
    "                weight = weights['nn_weight']\n",
    "            elif model_name == 'SVM' and weights['svm_weight'] > 0:\n",
    "                weight = weights['svm_weight']\n",
    "            elif model_name == 'XGBOOST' and weights['xgb_weight'] > 0:\n",
    "                weight = weights['xgb_weight']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if pred_class not in weighted_votes:\n",
    "                weighted_votes[pred_class] = 0\n",
    "            weighted_votes[pred_class] += weight * pred_prob\n",
    "        \n",
    "        # Find class with highest weighted vote\n",
    "        if weighted_votes:\n",
    "            best_class = max(weighted_votes.keys(), key=lambda k: weighted_votes[k])\n",
    "            best_prob = weighted_votes[best_class]\n",
    "        else:\n",
    "            best_class = \"Unknown\"\n",
    "            best_prob = 0.0\n",
    "        \n",
    "        ensemble_preds.append(best_class)\n",
    "        ensemble_probs[i] = best_prob\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'sample_name': sample_names,\n",
    "        'sample_index': range(len(sample_names)),\n",
    "        'prediction': ensemble_preds,\n",
    "        'prediction_prob': ensemble_probs,\n",
    "        'prediction_passed_cutoff': False  # Will be filled later with cutoffs\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def predict_ensemble_ovr(individual_predictions, ensemble_weights, sample_names):\n",
    "    \"\"\"\n",
    "    Make predictions using OvR ensemble method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    individual_predictions : dict\n",
    "        Dictionary containing pre-computed individual model predictions\n",
    "    ensemble_weights : dict\n",
    "        Dictionary containing ensemble weights\n",
    "    sample_names : list\n",
    "        Sample identifiers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : pd.DataFrame\n",
    "        DataFrame with ensemble predictions\n",
    "    \"\"\"\n",
    "    print(\"Making OvR Ensemble predictions...\")\n",
    "    \n",
    "    ovr_weights = ensemble_weights['ovr']\n",
    "    \n",
    "    # For each sample, calculate ensemble prediction\n",
    "    ensemble_preds = []\n",
    "    ensemble_probs = []\n",
    "    \n",
    "    for i in range(len(sample_names)):\n",
    "        class_scores = {}\n",
    "        \n",
    "        # For each class, calculate weighted ensemble score\n",
    "        for _, weight_row in ovr_weights.iterrows():\n",
    "            class_name = weight_row['class']\n",
    "            \n",
    "            # Map class name to match label mappings\n",
    "            class_mapped = class_name.replace('.', ' ')\n",
    "            \n",
    "            total_score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            # Add weighted contributions from each model\n",
    "            for model_name, results in individual_predictions.items():\n",
    "                if results is None:\n",
    "                    continue\n",
    "                    \n",
    "                pred_class = results.iloc[i]['prediction']\n",
    "                pred_prob = results.iloc[i]['prediction_prob']\n",
    "                \n",
    "                if model_name == 'NN':\n",
    "                    weight = weight_row['nn_weight']\n",
    "                elif model_name == 'SVM':\n",
    "                    weight = weight_row['svm_weight']\n",
    "                elif model_name == 'XGBOOST':\n",
    "                    weight = weight_row['xgb_weight']\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # If this model predicted this class, add weighted score\n",
    "                if pred_class == class_mapped and weight > 0:\n",
    "                    total_score += weight * pred_prob\n",
    "                    total_weight += weight\n",
    "            \n",
    "            if total_weight > 0:\n",
    "                class_scores[class_mapped] = total_score / total_weight\n",
    "            else:\n",
    "                class_scores[class_mapped] = 0\n",
    "        \n",
    "        # Find class with highest score\n",
    "        if class_scores:\n",
    "            best_class = max(class_scores.keys(), key=lambda k: class_scores[k])\n",
    "            best_prob = class_scores[best_class]\n",
    "        else:\n",
    "            best_class = \"Unknown\"\n",
    "            best_prob = 0.0\n",
    "        \n",
    "        ensemble_preds.append(best_class)\n",
    "        ensemble_probs.append(best_prob)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'sample_name': sample_names,\n",
    "        'sample_index': range(len(sample_names)),\n",
    "        'prediction': ensemble_preds,\n",
    "        'prediction_prob': ensemble_probs,\n",
    "        'prediction_passed_cutoff': False  # Will be filled later with cutoffs\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def apply_cutoffs(predictions_dict, cutoffs):\n",
    "    \"\"\"\n",
    "    Apply probability cutoffs to predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_dict : dict\n",
    "        Dictionary of prediction DataFrames\n",
    "    cutoffs : dict\n",
    "        Dictionary of cutoffs for each model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_dict : dict\n",
    "        Updated dictionary with cutoff information\n",
    "    \"\"\"\n",
    "    print(\"Applying probability cutoffs...\")\n",
    "    \n",
    "    # Map model names to cutoff keys\n",
    "    cutoff_mapping = {\n",
    "        'NN': 'neural_net',\n",
    "        'SVM': 'svm',\n",
    "        'XGBOOST': 'xgboost',\n",
    "        'Global_Ensemble': 'Global_Optimized',\n",
    "        'OvR_Ensemble': 'OvR_Ensemble'\n",
    "    }\n",
    "    \n",
    "    for model_name, df in predictions_dict.items():\n",
    "        cutoff_key = cutoff_mapping.get(model_name, model_name)\n",
    "        \n",
    "        if cutoff_key in cutoffs:\n",
    "            cutoff_value = cutoffs[cutoff_key]\n",
    "            df['prediction_passed_cutoff'] = df['prediction_prob'] >= cutoff_value\n",
    "            print(f\"Applied cutoff {cutoff_value:.2f} to {model_name}\")\n",
    "        else:\n",
    "            print(f\"No cutoff found for {model_name}\")\n",
    "            df['prediction_passed_cutoff'] = True  # Default to True if no cutoff\n",
    "    \n",
    "    return predictions_dict\n",
    "\n",
    "\n",
    "def save_predictions(predictions_dict, output_dir):\n",
    "    \"\"\"\n",
    "    Save prediction DataFrames to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_dict : dict\n",
    "        Dictionary of prediction DataFrames\n",
    "    output_dir : str\n",
    "        Output directory path\n",
    "    \"\"\"\n",
    "    print(f\"Saving predictions to {output_dir}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for model_name, df in predictions_dict.items():\n",
    "        filename = f\"{model_name}_predictions.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {model_name} predictions to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b8e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new samples from ../data/MLL_lab/STAR_AML_MLLlab.csv\n",
      "Loading training gene order from ../data/counts_20aug25.csv\n",
      "Found 60660 genes in training data\n",
      "Loaded 685 samples with 60660 genes\n",
      "Training data expects 60660 genes\n",
      "Missing genes: 0\n",
      "Extra genes: 0\n",
      "Reordered data shape: (685, 60660)\n",
      "Loading NN model...\n",
      "  Loading cached pipeline for n_genes=5000: ../data/out/final_models/pipelines/pipeline_ngenes_5000.pkl\n",
      "Loading SVM models...\n",
      "  Loading cached pipeline for n_genes=2000: ../data/out/final_models/pipelines/pipeline_ngenes_2000.pkl\n",
      "  Loading cached pipeline for n_genes=3000: ../data/out/final_models/pipelines/pipeline_ngenes_3000.pkl\n",
      "  Loading cached pipeline for n_genes=1000: ../data/out/final_models/pipelines/pipeline_ngenes_1000.pkl\n",
      "Loading XGBOOST models...\n",
      "\n",
      "Loaded 4 unique pipelines for n_genes: [1000, 2000, 3000, 5000]\n",
      "Loaded global ensemble weights\n",
      "Loaded OvR ensemble weights\n",
      "Loaded cutoffs for 5 models\n"
     ]
    }
   ],
   "source": [
    "# Set default paths if not provided\n",
    "base_path = \"..\"\n",
    "models_dir = \"../data/out/final_models\"\n",
    "weights_dir = \"../data/out/final_train_test/ensemble_weights/ensemble_weights\"\n",
    "cutoffs_file = \"../data/out/final_train_test/cutoffs/train_test_cutoffs.csv\"\n",
    "pipelines_dir = \"../data/out/final_models/pipelines\"\n",
    "\n",
    "# Load new samples\n",
    "X, sample_names = load_new_samples(\"../data/MLL_lab/STAR_AML_MLLlab.csv\")\n",
    "\n",
    "# Load models and metadata\n",
    "models = load_models_and_metadata(models_dir, pipelines_dir)\n",
    "\n",
    "# Load ensemble weights\n",
    "ensemble_weights = load_ensemble_weights(weights_dir)\n",
    "\n",
    "# Load cutoffs\n",
    "cutoffs = load_cutoffs(cutoffs_file)\n",
    "\n",
    "# Make predictions with all models\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d1fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making NN predictions...\n",
      "Making SVM predictions...\n",
      "  Processed data for pipeline 6294858752 (used by 6 classes)\n",
      "  Processed data for pipeline 6295455840 (used by 4 classes)\n",
      "  Processed data for pipeline 6312000688 (used by 9 classes)\n",
      "  Processed data for pipeline 6295555248 (used by 2 classes)\n",
      "Making XGBOOST predictions...\n",
      "  Processed data for pipeline 6294858752 (used by 8 classes)\n",
      "  Processed data for pipeline 6312000688 (used by 7 classes)\n",
      "  Processed data for pipeline 6295455840 (used by 6 classes)\n",
      "Making Global Ensemble predictions...\n",
      "Making OvR Ensemble predictions...\n",
      "Applying probability cutoffs...\n",
      "Applied cutoff 0.60 to NN\n",
      "Applied cutoff 0.59 to SVM\n",
      "Applied cutoff 0.45 to XGBOOST\n",
      "Applied cutoff 0.38 to Global_Ensemble\n",
      "Applied cutoff 0.48 to OvR_Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Individual model predictions\n",
    "if 'NN' in models:\n",
    "    predictions['NN'] = predict_nn_standard(X, models, sample_names)\n",
    "\n",
    "if 'SVM' in models:\n",
    "    predictions['SVM'] = predict_ovr_models(X, models, 'SVM', sample_names)\n",
    "\n",
    "if 'XGBOOST' in models:\n",
    "    predictions['XGBOOST'] = predict_ovr_models(X, models, 'XGBOOST', sample_names)\n",
    "\n",
    "# Ensemble predictions (reuse individual predictions to avoid redundant computation)\n",
    "if 'global' in ensemble_weights:\n",
    "    predictions['Global_Ensemble'] = predict_ensemble_global(predictions, ensemble_weights, sample_names)\n",
    "\n",
    "if 'ovr' in ensemble_weights:\n",
    "    predictions['OvR_Ensemble'] = predict_ensemble_ovr(predictions, ensemble_weights, sample_names)\n",
    "\n",
    "# Apply cutoffs\n",
    "predictions = apply_cutoffs(predictions, cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b3a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions to ../data/out/final_models/predictions\n",
      "Saved NN predictions to NN_predictions.csv\n",
      "Saved SVM predictions to SVM_predictions.csv\n",
      "Saved XGBOOST predictions to XGBOOST_predictions.csv\n",
      "Saved Global_Ensemble predictions to Global_Ensemble_predictions.csv\n",
      "Saved OvR_Ensemble predictions to OvR_Ensemble_predictions.csv\n",
      "\n",
      "=== Prediction Summary ===\n",
      "NN: 592/685 predictions passed cutoff\n",
      "SVM: 524/685 predictions passed cutoff\n",
      "XGBOOST: 545/685 predictions passed cutoff\n",
      "Global_Ensemble: 590/685 predictions passed cutoff\n",
      "OvR_Ensemble: 168/685 predictions passed cutoff\n",
      "\n",
      "Prediction pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "output_dir = \"../data/out/final_models/predictions\"\n",
    "save_predictions(predictions, output_dir)\n",
    "\n",
    "print(\"\\n=== Prediction Summary ===\")\n",
    "for model_name, df in predictions.items():\n",
    "    n_passed = df['prediction_passed_cutoff'].sum()\n",
    "    n_total = len(df)\n",
    "    print(f\"{model_name}: {n_passed}/{n_total} predictions passed cutoff\")\n",
    "\n",
    "print(\"\\nPrediction pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
