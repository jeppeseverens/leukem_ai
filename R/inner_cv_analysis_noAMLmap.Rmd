---
title: "inner_cv_analysis"
output: html_document
date: "2025-08-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
load_library_quietly <- function(package_name) {
  invisible(capture.output(
    suppressMessages(
      suppressWarnings(
        library(package_name, character.only = TRUE)
      )
    )
  ))
}

load_library_quietly("plyr")
load_library_quietly("dplyr")
load_library_quietly("stringr")
load_library_quietly("caret")
```

```{r}
# Filters
DATA_FILTERS <- list(
  min_samples_per_subtype = 10,
  excluded_subtypes = c("AML NOS", "Missing data", "Multi"),
  selected_studies = c(
    "TCGA-LAML",
    "LEUCEGENE", 
    "BEATAML1.0-COHORT",
    "AAML0531",
    "AAML1031"
  )
)

# Load mapping of class labels to numeric labels
label_mapping <- read.csv("~/Documents/AML_PhD/leukem_ai/label_mapping_15aug2025.csv")

# Load leukemia subtype data
leukemia_subtypes <- read.csv("../data_old/rgas_noAMLmap.csv")$ICC_Subtype

# Load study metadata
meta <- read.csv("../data_old/meta_noAMLmap.csv") 
study_names <- meta$Studies

# Filter data based on criteria
subtypes_with_sufficient_samples <- names(which(table(leukemia_subtypes) >= DATA_FILTERS$min_samples_per_subtype))
filter <- which(
  leukemia_subtypes %in% subtypes_with_sufficient_samples & 
    !leukemia_subtypes %in% DATA_FILTERS$excluded_subtypes & 
    study_names %in% DATA_FILTERS$selected_studies
)

# Load leukemia subtype data
filtered_leukemia_subtypes <- leukemia_subtypes[filter]

# Load study metadata
filtered_study_names <- study_names[filter]

meta <- meta[filter, ]
```


```{r}
MODEL_CONFIGS <- list(
  svm = list(
    classification_type = "OvR",
    file_paths = list(
      cv = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/out_2/svm_cv_19aug25/",
      loso = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/out_2/svm_loso_19aug25/"
    ),
    output_dir = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/inner_cv_best_params_n10/SVM_19aug"
  ),
  xgboost = list(
    classification_type = "OvR",
    file_paths = list(
      cv = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/out_2/xgb_cv_19aug25/",
      loso = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/out_2/xgb_loso_19aug25/"
    ),
    output_dir = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/inner_cv_best_params_n10/XGBOOST_19aug"
  ),
  neural_net = list(
    classification_type = "standard",
    file_paths = list(
      cv = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/out_2/nn_cv_19aug25/",
      loso = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/out_2/nn_loso_19aug25/"
    ),
    output_dir = "/Users/jsevere2/Documents/AML_PhD/leukem_ai/inner_cv_best_params_n10/NN_19aug"
  )
)

#' Combine multiple CSV files from a directory into a single data frame
#' @param directory_path Path to directory containing CSV files
#' @return Combined data frame
combine_csv_files <- function(directory_path) {
  if (!dir.exists(directory_path)) {
    stop(sprintf("Directory does not exist: %s", directory_path))
  }
  
  csv_files <- list.files(directory_path, recursive = TRUE, full.names = TRUE, pattern = "\\.csv$")
  
  if (length(csv_files) == 0) {
    stop(sprintf("No CSV files found in directory: %s", directory_path))
  }
  
  combined_results <- lapply(csv_files, function(file) {
    data.frame(data.table::fread(file, sep = ",", drop = 1))
  })
  
  # Remove NULL results from failed reads
  combined_results <- combined_results[!sapply(combined_results, is.null)]
  
  if (length(combined_results) == 0) {
    stop("No files could be read successfully")
  }
  
  do.call(rbind, combined_results)
}

#' Clean and parse string data consistently
#' @param input_string String to clean and parse
#' @return Numeric vector
parse_numeric_string <- function(input_string) {
  if (is.null(input_string) || is.na(input_string) || input_string == "") {
    return(numeric(0))
  }
  
  cleaned_string <- input_string %>%
    str_replace_all(",|\\[|\\]|\\{|\\}|\\\n", "") %>%
    str_squish()
  
  if (cleaned_string == "") {
    return(numeric(0))
  }
  
  numeric_values <- as.numeric(unlist(strsplit(cleaned_string, " ")))
  numeric_values[!is.na(numeric_values)]
}


#' Add descriptive class labels to data frame
#' @param data_frame Input data frame
#' @param label_mapping Label mapping data frame
#' @return Data frame with added class labels
add_class_labels <- function(data_frame, label_mapping) {
  if (!"class" %in% colnames(data_frame)) {
    stop("Data frame must contain 'class' column")
  }
  
  data_frame$class_label <- label_mapping$Label[data_frame$class + 1]
  data_frame
}

#' Process neural network results to clean epoch information
#' @param nn_results Neural network results data frame
#' @return Processed data frame
process_neural_net_results <- function(nn_results) {
  # Extract best_epoch as numeric
  nn_results$epochs <- str_match(nn_results$params, "best_epoch': np\\.int64\\((\\d+)\\)")[,2] |> as.integer()
  
  # Remove best_epoch from param string
  nn_results$params <- gsub(", 'best_epoch'.+", "", nn_results$params)
  
  # Add mean best_epoch per group back into param string
  nn_results %>%
    group_by(outer_fold, params) %>%
    mutate(params = paste0(params, ", 'best_epoch': ", round(mean(epochs)), "}")) %>%
    ungroup()
}

load_all_model_data <- function(model_configs) {
  cat("Loading model data...\n")
  
  model_results <- list()
  
  for (model_name in names(model_configs)) {
    config <- model_configs[[model_name]]
    cat(sprintf("Loading %s data...\n", toupper(model_name)))
    
    model_results[[model_name]] <- list()
    
    for (fold_type in names(config$file_paths)) {
      file_path <- config$file_paths[[fold_type]]
      results <- combine_csv_files(file_path)
      if (model_name == "neural_net") {
        if (!is.null(results)) {
          results <- process_neural_net_results(results)
        }
      } 
      
      if (!is.null(results)) {
        model_results[[model_name]][[fold_type]] <- results
      } else {
        warning(sprintf("Failed to load %s %s data", model_name, fold_type))
      }
    }
  }
  
  model_results
}

model_results <- load_all_model_data(MODEL_CONFIGS)
```

```{r}
test <- model_results$neural_net$loso %>% group_by(across(c("inner_fold","outer_fold", "params"))) %>%
    summarise(
      mean_kappa = mean(kappa, na.rm = TRUE),
      sd_kappa = sd(kappa, na.rm = TRUE),
      mean_mcc = mean(mcc, na.rm = TRUE),
      sd_mcc = sd(mcc, na.rm = TRUE),
      mean_accuracy = mean(accuracy, na.rm = TRUE),
      sd_accuracy = sd(accuracy, na.rm = TRUE),
      across(any_of(c("class_0", "class_1")), first),
      .groups = "drop_last"
    ) %>% group_by(across(c("inner_fold","outer_fold"))) %>%
    filter(mean_kappa == max(mean_kappa, na.rm = TRUE)) %>%
    slice(1) %>%
    ungroup()
```


```{r}
#' Extract the best hyperparameters per outer fold based on mean kappa
#' @param inner_cv_results Data frame with inner cross-validation results
#' @param classification_type Classification type: "standard", "OvR", or "OvO"
#' @return Data frame with best parameters for each outer fold
extract_best_hyperparameters <- function(inner_cv_results, classification_type) {
  # Validate inputs
  required_cols <- c("outer_fold", "params", "kappa", "accuracy")
  missing_cols <- setdiff(required_cols, colnames(inner_cv_results))
  if (length(missing_cols) > 0) {
    stop(sprintf("Missing required columns: %s", paste(missing_cols, collapse = ", ")))
  }
  
  # Choose grouping variables based on classification type
  grouping_vars <- if (classification_type == "standard") {
    c("outer_fold", "params")
  } else {
    c("outer_fold", "class", "params")
  }
  
  # Step 1: Compute mean kappa and accuracy across inner folds for each param set
  best_parameters <- inner_cv_results %>%
    group_by(across(all_of(grouping_vars))) %>%
    summarise(
      mean_kappa = mean(kappa, na.rm = TRUE),
      sd_kappa = sd(kappa, na.rm = TRUE),
      mean_mcc = mean(mcc, na.rm = TRUE),
      sd_mcc = sd(mcc, na.rm = TRUE),
      mean_accuracy = mean(accuracy, na.rm = TRUE),
      sd_accuracy = sd(accuracy, na.rm = TRUE),
      across(any_of(c("class_0", "class_1")), first),
      .groups = "drop_last"
    )
  
  # Step 2: For each group, retain the param set with the highest mean_kappa
  best_parameters %>%
    group_by(across(all_of(grouping_vars[-length(grouping_vars)]))) %>%
    filter(mean_kappa == max(mean_kappa, na.rm = TRUE)) %>%
    slice(1) %>%
    ungroup()
}

extract_all_best_parameters <- function(model_results, model_configs) {
  cat("Extracting best parameters...\n")
  
  best_parameters <- list()
  
  for (model_name in names(model_results)) {
    config <- model_configs[[model_name]]
    cat(sprintf("Extracting best parameters for %s...\n", toupper(model_name)))
    
    best_parameters[[model_name]] <- list()
    
    for (fold_type in names(model_results[[model_name]])) {
      results <- model_results[[model_name]][[fold_type]]
      if (!is.null(results)) {
        best_params <- extract_best_hyperparameters(results, config$classification_type)
        best_parameters[[model_name]][[fold_type]] <- best_params
      }
    }
  }
  
  best_parameters
}

# Extract best parameters
best_parameters <- extract_all_best_parameters(model_results, MODEL_CONFIGS)
```



```{r}
#' Modify class labels to group related subtypes
#' @param vector Vector of class labels
#' @return Modified vector with grouped classes
modify_classes <- function(vector) {
  vector[grepl("MDS|TP53", vector)] <- "MDS.r"
  vector[!grepl("MLLT3", vector) & grepl("KMT2A", vector)] <- "other.KMT2A"
  vector
}

#' Save best parameters for all models
#' @param best_parameters Best parameters for each model
#' @param model_configs Model configurations
save_all_best_parameters <- function(best_parameters, model_configs) {
  cat("Saving best parameters...\n")
  
  for (model_name in names(best_parameters)) {
    config <- model_configs[[model_name]]
    output_dir <- config$output_dir
    
    cat(sprintf("Saving %s results...\n", toupper(model_name)))
    dir.create(output_dir)
    
    for (fold_type in names(best_parameters[[model_name]])) {
      best_params <- best_parameters[[model_name]][[fold_type]]
      if (!is.null(best_params)) {
        filename <- sprintf("%s_best_param_%s.csv", toupper(model_name), fold_type)
        filepath <- file.path(output_dir, filename)
        write.csv(best_params, file = filepath, row.names = FALSE)
        cat(sprintf("  Saved: %s\n", filepath))
      }
    }
  }
}

# Save best parameters
save_all_best_parameters(best_parameters, MODEL_CONFIGS)
```


```{r}
#' Ensure all required class columns exist in probability matrix
#' @param prob_matrix Probability matrix
#' @param label_mapping Label mapping data frame
#' @return Matrix with all required columns
ensure_all_class_columns <- function(prob_matrix, label_mapping) {
  required_cols <- make.names(label_mapping$Label)
  missing_cols <- setdiff(required_cols, colnames(prob_matrix))
  
  for (col_name in missing_cols) {
    prob_matrix[[col_name]] <- 0
  }
  
  prob_matrix[, required_cols, drop = FALSE]
}

#' Generate probability data frames for One-vs-Rest classification
#' @param cv_results_df Cross-validation results data frame
#' @param best_params_df Best parameters data frame
#' @param label_mapping Label mapping data frame
#' @return List of probability data frames organized by outer fold
generate_ovr_probability_matrices <- function(cv_results_df, best_params_df, label_mapping) {
  best_params_with_labels <- add_class_labels(best_params_df, label_mapping)
  outer_fold_ids <- unique(cv_results_df$outer_fold)
  
  probability_matrices <- list()
  
  for (outer_fold_id in outer_fold_ids) {
    outer_fold_data <- cv_results_df[cv_results_df$outer_fold == outer_fold_id, ]
    inner_fold_ids <- unique(outer_fold_data$inner_fold)
    
    fold_matrices <- list()
    
    for (inner_fold_id in inner_fold_ids) {
      inner_fold_data <- outer_fold_data[outer_fold_data$inner_fold == inner_fold_id, ]
      class_labels <- unique(inner_fold_data$class_label)
      
      # Skip if no data or invalid predictions
      if (nrow(inner_fold_data) == 0 || 
          is.null(inner_fold_data$preds_prob[1]) || 
          is.na(inner_fold_data$preds_prob[1])) {
        next
      }
      
      num_samples <- length(parse_numeric_string(inner_fold_data$preds_prob[1]))
      if (num_samples == 0) next
      
      probability_matrix <- matrix(NA, nrow = num_samples, ncol = length(class_labels))
      colnames(probability_matrix) <- class_labels
      true_labels_vector <- rep(NA, num_samples)
      
      for (j in seq_along(class_labels)) {
        current_class_label <- class_labels[j]
        best_param_for_class <- best_params_with_labels[
          best_params_with_labels$outer_fold == outer_fold_id & 
          best_params_with_labels$class_label == current_class_label,
        ]$params
        
        if (length(best_param_for_class) == 0) next
        
        best_param_row <- inner_fold_data[
          inner_fold_data$class_label == current_class_label & 
          inner_fold_data$params == best_param_for_class,
        ]
        
        if (nrow(best_param_row) == 0) next
        
        probs <- parse_numeric_string(best_param_row$preds_prob)
        if (length(probs) == num_samples) {
          probability_matrix[, j] <- probs
        }
        
        target_values <- parse_numeric_string(best_param_row$y_val)
        true_labels_vector[target_values == 1] <- current_class_label
      }
      
      if (all(is.na(true_labels_vector))) next
      
      probability_matrix <- t(apply(probability_matrix, 1, function(row) row / sum(row)))
      probability_matrix <- data.frame(probability_matrix)
      probability_matrix <- ensure_all_class_columns(probability_matrix, label_mapping)
      
      # Add true labels
      probability_matrix$y <- make.names(true_labels_vector)
      # Add inner fold name
      probability_matrix$inner_fold <- inner_fold_id
      # Add outer fold name
      probability_matrix$outer_fold <- outer_fold_id
      probability_matrix$indices <- parse_numeric_string(inner_fold_data$sample_indices[1]) + 1
      probability_matrix$study <- study_names[probability_matrix$indices]
      fold_matrices[[as.character(inner_fold_id)]] <- probability_matrix
    }
    
    if (length(fold_matrices) > 0) {
      probability_matrices[[as.character(outer_fold_id)]] <- do.call(rbind, fold_matrices)
      probability_matrices[[as.character(outer_fold_id)]][is.na(probability_matrices[[as.character(outer_fold_id)]])] <- 0
    }
  }
  
  probability_matrices
}

#' Generate probability data frames for standard multiclass classification
#' @param cv_results_df Cross-validation results data frame
#' @param best_params_df Best parameters data frame
#' @param label_mapping Label mapping data frame
#' @param filtered_subtypes Filtered leukemia subtypes
#' @return List of probability data frames organized by outer fold

generate_standard_probability_matrices <- function(cv_results_df, best_params_df, label_mapping, filtered_subtypes) {
  outer_fold_ids <- unique(cv_results_df$outer_fold)
  probability_matrices <- list()
  
  for (outer_fold_id in outer_fold_ids) {
    outer_fold_data <- cv_results_df[cv_results_df$outer_fold == outer_fold_id, ]
    inner_fold_ids <- unique(outer_fold_data$inner_fold)
    
    fold_matrices <- list()
    
    for (inner_fold_id in inner_fold_ids) {
      best_param <- best_params_df[best_params_df$outer_fold == outer_fold_id, ]$params
      
      
      inner_fold_data <- outer_fold_data[
        outer_fold_data$inner_fold == inner_fold_id & 
        outer_fold_data$params == best_param, 
      ]
      
      
      class_indices <- unique(parse_numeric_string(inner_fold_data$classes))
      class_labels <- label_mapping$Label[class_indices + 1]
      
      num_samples <- length(parse_numeric_string(inner_fold_data$sample_indices))
      
      probs <- parse_numeric_string(inner_fold_data$preds_prob)
      
      probability_matrix <- t(matrix(probs, ncol = num_samples, nrow = length(class_labels)))
      colnames(probability_matrix) <- make.names(class_labels)
      
      probability_matrix <- data.frame(probability_matrix)
      probability_matrix <- ensure_all_class_columns(probability_matrix, label_mapping)
      
      probability_matrix <- t(apply(probability_matrix, 1, function(row) row / sum(row)))
      probability_matrix <- data.frame(probability_matrix)
      
      sample_indices <- parse_numeric_string(inner_fold_data$sample_indices)
  
      probability_matrix$y <- make.names(filtered_subtypes[sample_indices + 1])

      probability_matrix <- data.frame(probability_matrix)
      probability_matrix$inner_fold <- inner_fold_id # this is the left out fold for the inner cv
      probability_matrix$outer_fold <- outer_fold_id # outer left out fold, more an id of the cv run
      probability_matrix$indices <- parse_numeric_string(inner_fold_data$sample_indices) + 1
      probability_matrix$study <- study_names[probability_matrix$indices]
      fold_matrices[[as.character(inner_fold_id)]] <- probability_matrix
    }
    
    probability_matrices[[as.character(outer_fold_id)]] <- do.call(rbind, fold_matrices)
  }
  
  probability_matrices
}

probability_matrices_1 <- list()
probability_matrices_2 <- list()
for (model_name in names(model_results)) {
    config <- MODEL_CONFIGS[[model_name]]
    cat(sprintf("Extracting %s probabilities...\n", toupper(model_name)))
    
    probability_matrices_1[[model_name]] <- list()
    
    for (fold_type in names(model_results[[model_name]])) {
      results <- model_results[[model_name]][[fold_type]]
      best_params <- best_parameters[[model_name]][[fold_type]]
      
      if (!is.null(results) && !is.null(best_params)) {
        if (config$classification_type == "OvR") {
          probs <- generate_ovr_probability_matrices(results, best_params, label_mapping)
        } else {
          probs <- generate_standard_probability_matrices(results, best_params, label_mapping, filtered_leukemia_subtypes)
        }
        probability_matrices_1[[model_name]][[fold_type]] <- probs
        probs_2 <- lapply(probs, function(prob) {
          MDS_cols <- grepl("MDS|TP53", colnames(prob))
          MDS <- rowSums(prob[,MDS_cols])
          
          other_KMT2A_cols <- grepl("KMT2A", colnames(prob)) & !grepl("MLLT3", colnames(prob))
          other_KMT2A <- rowSums(prob[,other_KMT2A_cols])
          
          prob <- prob[, !(MDS_cols | other_KMT2A_cols)]
          prob$MDS.r <- MDS
          prob$other.KMT2A <- other_KMT2A
          prob$y <- modify_classes(prob$y)
          colnames(prob) <- modify_classes(colnames(prob))
          return(prob)
        })
        
        probability_matrices_2[[model_name]][[fold_type]] <- probs_2
        remove(probs)
      }
    }
}
```

```{r}
performance_results <- list()
results_list <- list()

get_per_model_performance <- function(probability_matrices){
for (model in names(probability_matrices)) {
    probability_matrices_model <- probability_matrices[[model]]
    
  for (method in names(probability_matrices_model)) {
    probability_matrices_method <- probability_matrices_model[[method]]
    
    for (fold_name in names(probability_matrices_method)){
    
    # Get matrix for this fold
    the_matrix <- probability_matrices_method[[fold_name]]
    
    # Extract true labels and remove from probability matrix
    truth <- the_matrix$y
    prob_matrix <- the_matrix[, !colnames(the_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
    
    # Get predictions
    preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
    
    # Clean class labels
    truth <- gsub("Class. ", "", truth)
    preds <- gsub("Class. ", "", preds)
    truth <- modify_classes(truth)
    preds <- modify_classes(preds)
    # Ensure all classes are represented
    all_classes <- unique(c(truth, preds))
    truth <- factor(truth, levels = all_classes)
    preds <- factor(preds, levels = all_classes)
    
    # Calculate confusion matrix and metrics
    cm <- caret::confusionMatrix(preds, truth)
    mcc <- mltools::mcc(preds, truth)
    
    results_list[[length(results_list) + 1]] <- data.frame(
        model = model,
        method = method,
        fold_name = fold_name,
        kappa = cm[["overall"]][["Kappa"]],
        accuracy = cm[["overall"]][["Accuracy"]],
        mcc = mcc,
        confusion_matrix = I(list(cm))  # store cm object in a column
    )
  }
  }
}
results_df <- do.call(rbind, results_list)
results_df
}
library(dplyr)
get_per_model_performance(probability_matrices_1) %>% group_by(model, method) %>% summarise(mean_kappa = mean(kappa), 
                                                                           sd_kappa = sd(kappa),
                                                                           mean_mcc = mean(mcc),
                                                                           sd_mcc = sd(mcc))

get_per_model_performance(probability_matrices_2) %>% group_by(model, method) %>% summarise(mean_kappa = mean(kappa), 
                                                                           sd_kappa = sd(kappa),
                                                                           mean_mcc = mean(mcc),
                                                                           sd_mcc = sd(mcc))
```


```{r}
generate_weights <- function(step = 0.3){
  # Generate all combinations of weights from 0 to 1 in 0.1 steps
  steps <- seq(0, 1, by = step)
  grid <- expand.grid(SVM = steps, XGB = steps, NN = steps)
  
  # Remove rows where all the values are the same
  grid <- grid[!apply(grid, 1, function(x) length(unique(x)) == 1),]
  grid <- t(apply(grid, 1, function(row) round(row / sum(row), 1)))
  grid <- grid[!duplicated(grid), ]
  
  # Convert to a named list
  ENSEMBLE_WEIGHTS <- apply(grid, 1, function(row) {
    list(SVM = row["SVM"], XGB = row["XGB"], NN = row["NN"])
  })
  
  # Name the list elements for clarity (optional)
  names(ENSEMBLE_WEIGHTS) <- paste0("W", seq_along(ENSEMBLE_WEIGHTS))
  ENSEMBLE_WEIGHTS[["ALL"]] <- list(SVM = 1, XGB = 1, NN = 1)
  
  return(ENSEMBLE_WEIGHTS)
}

#' Perform One-vs-Rest ensemble analysis for each class separately
#' @param results Analysis results containing probability matrices
#' @param weights Weight configurations for ensemble
#' @param type Type of analysis ("cv" or "loso")
#' @return List of performance metrics for each fold, weight configuration, and class
perform_ovr_ensemble_analysis <- function(results, weights, type = "cv") {
  cat("Performing One-vs-Rest ensemble analysis...\n")
  
  folds <- names(results$probability_matrices$svm[[type]])
  df_list <- list()
  
  for (fold in folds) {
    cat(sprintf("  Processing fold %s...\n", fold))
    df_list[[fold]] <- list()
    
    # Align probability matrices for this fold
    aligned_matrices <- align_probability_matrices(results$probability_matrices, fold, type)
    if (is.null(aligned_matrices)) {
      cat(sprintf("    Skipping fold %s - unable to align matrices\n", fold))
      next
    }
    
    for (i in seq_along(weights)) {
      weight_i <- names(weights)[i]
      
      # Extract aligned probability data frames
      prob_df_SVM <- aligned_matrices$svm
      prob_df_XGB <- aligned_matrices$xgboost
      prob_df_NN <- aligned_matrices$neural_net
      truth <- aligned_matrices$truth
      
      # Get all class names
      all_classes <- colnames(prob_df_SVM)
      
      # Analyze each class separately as a binary classification problem
      for (class_name in all_classes) {
        # Calculate weighted ensemble probabilities for this class only
        class_probs <- prob_df_SVM[[class_name]] * weights[[weight_i]]$SVM +
                      prob_df_XGB[[class_name]] * weights[[weight_i]]$XGB +
                      prob_df_NN[[class_name]] * weights[[weight_i]]$NN
        
        # Create binary predictions: class vs not class
        # Threshold at 0.5 for binary classification
        binary_preds <- ifelse(class_probs > 0.5, "Class", "Not_Class")
        
        # Create binary truth: class vs not class
        binary_truth <- ifelse(truth == class_name, "Class", "Not_Class")
        
        # Ensure factor levels
        binary_truth <- factor(binary_truth, levels = c("Not_Class", "Class"))
        binary_preds <- factor(binary_preds, levels = c("Not_Class", "Class"))
        
        # Calculate binary confusion matrix and metrics
        cm <- caret::confusionMatrix(binary_preds, binary_truth, positive = "Class")
        
        # Extract binary performance metrics
        sensitivity <- cm$byClass["Sensitivity"]
        specificity <- cm$byClass["Specificity"]
        balanced_accuracy <- cm$byClass["Balanced Accuracy"]
        f1_score <- cm$byClass["F1"]
        
        # Create results data frame for this class
        df <- data.frame(
          weights = weight_i,
          type = type,
          class = gsub("Class.", "", class_name),
          sensitivity = sensitivity,
          specificity = specificity,
          balanced_accuracy = balanced_accuracy,
          f1_score = f1_score,
          stringsAsFactors = FALSE
        )
        
        df_list[[fold]][[paste(weight_i, class_name, sep = "_")]] <- df
      }
    }
    
    # Combine results for this fold
    df_list[[fold]] <- do.call(rbind, df_list[[fold]])
  }
  
  df_list
}

#' Generate One-vs-Rest optimized ensemble probability matrices
#' @param results Analysis results containing probability matrices
#' @param weights Weight configurations for ensemble
#' @param type Type of analysis ("cv" or "loso")
#' @return List containing optimized probability matrices and weights used for each fold
generate_ovr_optimized_ensemble_matrices <- function(results, weights, type = "cv", ensemble_performance) {
  cat("Generating One-vs-Rest optimized ensemble probability matrices...\n")
  
  folds <- names(results$probability_matrices$svm[[type]])
  optimized_matrices <- list()
  weights_used <- list()  # Store weights used for each fold
  
  for (fold in folds) {
    cat(sprintf("  Creating OvR optimized matrix for fold %s...\n", fold))
    
    # Align probability matrices for this fold
    aligned_matrices <- align_probability_matrices(results$probability_matrices, fold, type)
    if (is.null(aligned_matrices)) {
      cat(sprintf("    Skipping fold %s - unable to align matrices\n", fold))
      next
    }
    
    # Get best weight configuration for each class in this fold
    fold_performance <- ensemble_performance[[fold]]
    if (nrow(fold_performance) == 0) {
      cat(sprintf("    Skipping fold %s - no performance data available\n", fold))
      next
    }
    
    # Extract aligned probability data frames
    prob_df_SVM <- aligned_matrices$svm
    prob_df_XGB <- aligned_matrices$xgboost
    prob_df_NN <- aligned_matrices$neural_net
    truth <- aligned_matrices$truth
    
    # Get all class names
    all_classes <- colnames(prob_df_SVM)
    
    # Get classes that actually have performance data (i.e., were present in this fold)
    available_classes <- unique(fold_performance$class)
    
    # Initialize optimized probability matrix
    optimized_matrix <- matrix(0, nrow = nrow(prob_df_SVM), ncol = length(all_classes))
    colnames(optimized_matrix) <- all_classes
    
    # Store weights used for each class in this fold
    fold_weights_used <- list()
    
    # For each class, use the best ensemble weights based on F1 score
    for (class_name in all_classes) {
      # Clean class name for matching - try multiple variations
      clean_class_name <- gsub("Class.", "", class_name)
      clean_class_name_no_dots <- gsub("\\.", "", clean_class_name)
      
      # Check if this class has performance data (was present in this fold)
      class_has_data <- FALSE
      class_performance <- data.frame()
      
      # Try to find performance data for this class
      if (clean_class_name %in% available_classes) {
        class_performance <- fold_performance[fold_performance$class == clean_class_name, ]
        class_has_data <- TRUE
      } else if (clean_class_name_no_dots %in% available_classes) {
        class_performance <- fold_performance[fold_performance$class == clean_class_name_no_dots, ]
        class_has_data <- TRUE
      } else {
        # Try partial matching
        matching_classes <- available_classes[
          grepl(clean_class_name, available_classes, ignore.case = TRUE) |
          grepl(clean_class_name_no_dots, available_classes, ignore.case = TRUE)
        ]
        if (length(matching_classes) > 0) {
          class_performance <- fold_performance[fold_performance$class == matching_classes[1], ]
          class_has_data <- TRUE
        }
      }
      
      if (class_has_data && nrow(class_performance) > 0) {
        # Use the best weights found for this class
        best_weight_indices <- which.max(class_performance$f1_score)
        best_weight_name <- class_performance$weights[best_weight_indices]
        
        # Ensure we have a single weight name (take first if multiple)
        if (length(best_weight_name) > 1) {
          best_weight_name <- best_weight_name[1]
          cat(sprintf("    Warning: Multiple best weights found for class %s, using first one\n", clean_class_name))
        }
        
        # Debug: Check if best_weight_name is valid
        if (is.null(best_weight_name) || is.na(best_weight_name) || length(best_weight_name) == 0 || best_weight_name == "") {
          cat(sprintf("    Warning: Invalid weight name for class %s, using default weights\n", clean_class_name))
          best_weights <- weights[["ALL"]]
          best_weight_name <- "ALL"
        } else if (!best_weight_name %in% names(weights)) {
          cat(sprintf("    Warning: Weight '%s' not found in weights list for class %s, using default weights\n", best_weight_name, clean_class_name))
          best_weights <- weights[["ALL"]]
          best_weight_name <- "ALL"
        } else {
          best_weights <- weights[[best_weight_name]]
        }
        
        # Store the weight configuration used for this class
        fold_weights_used[[clean_class_name]] <- list(
          weight_name = best_weight_name,
          weights = best_weights,
          f1_score = max(class_performance$f1_score)
        )
        
        # Calculate weighted ensemble probabilities for this class
        class_probs <- prob_df_SVM[[class_name]] * best_weights$SVM +
                      prob_df_XGB[[class_name]] * best_weights$XGB +
                      prob_df_NN[[class_name]] * best_weights$NN
        
        optimized_matrix[, class_name] <- class_probs
      } else {
        # This class wasn't present in the performance data, so use default weights
        cat(sprintf("    Using default weights for class %s (not present in this fold)\n", clean_class_name))
        best_weights <- weights[["ALL"]]
        best_weight_name <- "ALL"
        
        # Store the default weight configuration used for this class
        fold_weights_used[[clean_class_name]] <- list(
          weight_name = best_weight_name,
          weights = best_weights,
          f1_score = NA
        )
        
        # Calculate weighted ensemble probabilities for this class
        class_probs <- prob_df_SVM[[class_name]] * best_weights$SVM +
                      prob_df_XGB[[class_name]] * best_weights$XGB +
                      prob_df_NN[[class_name]] * best_weights$NN
        
        optimized_matrix[, class_name] <- class_probs
      }
    }
    
    # Convert to data frame and add true labels
    optimized_matrix <- data.frame(optimized_matrix)
    optimized_matrix <- t(apply(optimized_matrix, 1, function(row) row / sum(row)))
    optimized_matrix <- data.frame(optimized_matrix)
    optimized_matrix$y <- truth
    
    optimized_matrices[[fold]] <- optimized_matrix
    weights_used[[fold]] <- fold_weights_used
  }
  
  # Return both matrices and weights used
  list(
    matrices = optimized_matrices,
    weights_used = weights_used
  )
}

#' Calculate multiclass performance from One-vs-Rest ensemble matrices
#' @param ovr_ensemble_result Result from generate_ovr_optimized_ensemble_matrices containing matrices and weights
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with multiclass performance metrics for each fold
analyze_ovr_ensemble_multiclass_performance <- function(ovr_ensemble_result, type = "cv") {
  cat("Analyzing One-vs-Rest ensemble multiclass performance...\n")
  
  # Extract optimized matrices from the result structure
  optimized_matrices <- ovr_ensemble_result$matrices
  
  performance_results <- list()
  
  for (fold_name in names(optimized_matrices)) {
    cat(sprintf("  Analyzing fold %s...\n", fold_name))
    
    # Get the optimized matrix for this fold
    optimized_matrix <- optimized_matrices[[fold_name]]
    
    # Extract true labels and remove from probability matrix
    truth <- optimized_matrix$y
    prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices","study"), drop = FALSE]
    
    # Get predictions (class with highest probability)
    preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
    
    # Clean class labels
    truth <- gsub("Class.", "", truth)
    preds <- gsub("Class.", "", preds)
    truth <- modify_classes(truth)
    preds <- modify_classes(preds)
    # Ensure all classes are represented
    all_classes <- unique(c(truth, preds))
    truth <- factor(truth, levels = all_classes)
    preds <- factor(preds, levels = all_classes)
    
    # Calculate confusion matrix and metrics
    cm <- caret::confusionMatrix(preds, truth)
    
    performance_results[[fold_name]] <- cm
  }
  
  # Combine all fold results
  performance_results
}

#' Perform global ensemble optimization using overall kappa
#' @param results Analysis results containing probability matrices
#' @param weights Weight configurations for ensemble
#' @param type Type of analysis ("cv" or "loso")
#' @return List of performance metrics for each fold and weight configuration
perform_global_ensemble_analysis <- function(results, weights, type = "cv") {
  cat("Performing global ensemble analysis...\n")
  
  folds <- names(results$probability_matrices$svm[[type]])
  df_list <- list()
  
  for (fold in folds) {
    cat(sprintf("  Processing fold %s...\n", fold))
    df_list[[fold]] <- list()
    
    # Align probability matrices for this fold
    aligned_matrices <- align_probability_matrices(results$probability_matrices, fold, type)
    if (is.null(aligned_matrices)) {
      cat(sprintf("    Skipping fold %s - unable to align matrices\n", fold))
      next
    }
    
    for (i in seq_along(weights)) {
      weight_i <- names(weights)[i]
      
      # Extract aligned probability data frames
      prob_df_SVM <- aligned_matrices$svm
      prob_df_XGB <- aligned_matrices$xgboost
      prob_df_NN <- aligned_matrices$neural_net
      truth <- aligned_matrices$truth
      
      # Calculate weighted ensemble probabilities
      prob_df <- prob_df_SVM * weights[[weight_i]]$SVM +
                 prob_df_XGB * weights[[weight_i]]$XGB +
                 prob_df_NN * weights[[weight_i]]$NN
      
      # Normalize probabilities to sum to 1 for each sample
      prob_df <- prob_df / rowSums(prob_df)
      
      # Get predictions
      preds <- colnames(prob_df)[apply(prob_df, 1, which.max)]
      
      # Clean class labels
      truth <- make.names(gsub("Class. ", "", truth))
      preds <- make.names(gsub("Class. ", "", preds))
      
      # Ensure all classes are represented
      all_classes <- unique(c(truth, preds))
      truth <- factor(truth, levels = all_classes)
      preds <- factor(preds, levels = all_classes)
      
      # Calculate confusion matrix and metrics
      cm <- caret::confusionMatrix(preds, truth)
      
      # Extract overall performance metrics
      overall_kappa <- cm$overall["Kappa"]
      overall_accuracy <- cm$overall["Accuracy"]
      
      # Create results data frame
      df <- data.frame(
        weights = weight_i,
        type = type,
        kappa = overall_kappa,
        accuracy = overall_accuracy
      )
      
      df_list[[fold]][[weight_i]] <- df
    }
    
    # Combine results for this fold
    df_list[[fold]] <- do.call(rbind, df_list[[fold]])
  }
  
  df_list
}

#' Generate globally optimized ensemble probability matrices
#' @param results Analysis results containing probability matrices
#' @param weights Weight configurations for ensemble
#' @param type Type of analysis ("cv" or "loso")
#' @return List containing optimized probability matrices and weights used for each fold
generate_global_optimized_ensemble_matrices <- function(results, weights, type = "cv", ensemble_performance) {
  cat("Generating globally optimized ensemble probability matrices...\n")
  
  folds <- names(results$probability_matrices$svm[[type]])
  optimized_matrices <- list()
  weights_used <- list()  # Store weights used for each fold
  
  for (fold in folds) {
    cat(sprintf("  Creating globally optimized matrix for fold %s...\n", fold))
    
    # Align probability matrices for this fold
    aligned_matrices <- align_probability_matrices(results$probability_matrices, fold, type)
    if (is.null(aligned_matrices)) {
      cat(sprintf("    Skipping fold %s - unable to align matrices\n", fold))
      next
    }
    
    # Get best weight configuration for this fold (highest kappa)
    fold_performance <- ensemble_performance[[fold]]
    if (nrow(fold_performance) == 0) {
      cat(sprintf("    Skipping fold %s - no performance data available\n", fold))
      next
    }
    
    best_weight_name <- fold_performance$weights[which.max(fold_performance$kappa)]
    best_weights <- weights[[best_weight_name]]
    best_kappa <- max(fold_performance$kappa)
    
    # Store the weight configuration used for this fold
    weights_used[[fold]] <- list(
      weight_name = best_weight_name,
      weights = best_weights,
      kappa = best_kappa
    )
    
    cat(sprintf("    Using globally optimized weights (%s) for fold %s (kappa = %.4f)\n", 
                best_weight_name, fold, best_kappa))
    
    # Extract aligned probability data frames
    prob_df_SVM <- aligned_matrices$svm
    prob_df_XGB <- aligned_matrices$xgboost
    prob_df_NN <- aligned_matrices$neural_net
    truth <- aligned_matrices$truth
    
    # Calculate weighted ensemble probabilities using best global weights
    optimized_matrix <- prob_df_SVM * best_weights$SVM +
                       prob_df_XGB * best_weights$XGB +
                       prob_df_NN * best_weights$NN
    
    # Normalize probabilities to sum to 1 for each sample
    optimized_matrix <- optimized_matrix / rowSums(optimized_matrix)
    
    # Convert to data frame and add true labels
    optimized_matrix <- data.frame(optimized_matrix)
    optimized_matrix$y <- truth
    
    optimized_matrices[[fold]] <- optimized_matrix
  }
  
  # Return both matrices and weights used
  list(
    matrices = optimized_matrices,
    weights_used = weights_used
  )
}

#' Calculate performance metrics for optimized ensemble matrices
#' @param ensemble_result Result containing optimized ensemble probability matrices and weights (for global optimization) or just matrices (for other methods)
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with performance metrics for each fold
analyze_optimized_ensemble_performance <- function(ensemble_result, type = "cv") {
  cat("Analyzing optimized ensemble performance...\n")
  
  # Handle different input structures - if it's the new structure with matrices and weights, extract matrices
  if (is.list(ensemble_result) && "matrices" %in% names(ensemble_result)) {
    optimized_matrices <- ensemble_result$matrices
  } else {
    # For backward compatibility with old structure
    optimized_matrices <- ensemble_result
  }
  
  performance_results <- list()
  
  for (fold_name in names(optimized_matrices)) {
    cat(sprintf("  Analyzing fold %s...\n", fold_name))
    
    # Get the optimized matrix for this fold
    optimized_matrix <- optimized_matrices[[fold_name]]
    
    # Extract true labels and remove from probability matrix
    truth <- optimized_matrix$y
    prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
    
    # Get predictions
    preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
    
    # Clean class labels
    truth <- gsub("Class. ", "", truth)
    preds <- gsub("Class. ", "", preds)
    truth <- modify_classes(truth)
    preds <- modify_classes(preds)
    # Ensure all classes are represented
    all_classes <- unique(c(truth, preds))
    truth <- factor(truth, levels = all_classes)
    preds <- factor(preds, levels = all_classes)
    
    # Calculate confusion matrix and metrics
    cm <- caret::confusionMatrix(preds, truth)
    
    performance_results[[fold_name]] <- cm
  }
  
  # Combine all fold results
  performance_results
}
# =============================================================================
# Matrix Alignment Functions
# =============================================================================

#' Align probability matrices from different models for ensemble analysis
#' @param prob_matrices List of probability matrices from different models
#' @param fold_name Name of the fold being processed
#' @param type Type of analysis ("cv" or "loso")
#' @return List of aligned probability matrices
align_probability_matrices <- function(prob_matrices, fold_name, type) {
  # Extract matrices for this fold
  svm_matrix <- prob_matrices$svm[[type]][[fold_name]]
  xgb_matrix <- prob_matrices$xgboost[[type]][[fold_name]]
  nn_matrix <- prob_matrices$neural_net[[type]][[fold_name]]
  
  # Check if all matrices exist
  if (is.null(svm_matrix) || is.null(xgb_matrix) || is.null(nn_matrix)) {
    warning(sprintf("Missing probability matrix for fold %s in %s analysis", fold_name, type))
    return(NULL)
  }
  
  # Extract true labels
  truth_svm <- make.names(svm_matrix$y)
  truth_xgb <- make.names(xgb_matrix$y)
  truth_nn <- make.names(nn_matrix$y)
  
  # Remove non-probability columns from matrices
  svm_matrix <- svm_matrix[, !colnames(svm_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
  xgb_matrix <- xgb_matrix[, !colnames(xgb_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
  nn_matrix <- nn_matrix[, !colnames(nn_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
  
  # Get all unique class names across all models
  all_classes <- unique(c(
    colnames(svm_matrix),
    colnames(xgb_matrix),
    colnames(nn_matrix)
  ))
  
  # Get the minimum number of samples across all models
  min_samples <- min(nrow(svm_matrix), nrow(xgb_matrix), nrow(nn_matrix))
  max_samples <- max(nrow(svm_matrix), nrow(xgb_matrix), nrow(nn_matrix))
  # Align matrices by ensuring they have the same columns and sample size
  aligned_matrices <- list()
  
  for (model_name in c("svm", "xgboost", "neural_net")) {
    matrix_data <- switch(model_name,
      "svm" = svm_matrix,
      "xgboost" = xgb_matrix,
      "neural_net" = nn_matrix
    )
    
    # Ensure all required columns exist (add 0s for missing classes)
    missing_cols <- setdiff(all_classes, colnames(matrix_data))
    for (col in missing_cols) {
      matrix_data[[col]] <- 0
    }
    
    # Reorder columns to match all_classes
    matrix_data <- matrix_data[, all_classes, drop = FALSE]
    
    if (nrow(matrix_data) < max_samples) {
      cat(sprintf("The probabilties for %s have less samples then max_samples\n", model_name))
    }
    
    # Truncate to minimum sample size if necessary
    if (nrow(matrix_data) > min_samples) {
      matrix_data <- matrix_data[1:min_samples, , drop = FALSE]
    }
    
    aligned_matrices[[model_name]] <- matrix_data
  }
  
  # Use the truth from SVM as reference (or the one with minimum samples)
  reference_truth <- if (length(truth_svm) >= min_samples) {
    truth_svm[1:min_samples]
  } else if (length(truth_xgb) >= min_samples) {
    truth_xgb[1:min_samples]
  } else {
    truth_nn[1:min_samples]
  }
  
  # Add aligned truth to the result
  aligned_matrices$truth <- reference_truth
  
  aligned_matrices
}


#' Run ensemble analysis for both CV and LOSO types
#' @param probability_matrices Probability matrices for all models
#' @param weights Weight configurations for ensemble
#' @return List of results for both CV and LOSO
run_ensemble_analysis_for_both_types <- function(probability_matrices, weights) {
  cat("Running ensemble analysis for both CV and LOSO...\n")
  
  results <- list()
  
  for (analysis_type in c("cv", "loso")) {
    cat(sprintf("\n=== Running %s analysis ===\n", toupper(analysis_type)))
    
    # Check if we have data for this analysis type
    if (!all(sapply(probability_matrices, function(x) analysis_type %in% names(x)))) {
      cat(sprintf("Skipping %s analysis - missing data\n", toupper(analysis_type)))
      next
    }

    # Perform global ensemble analysis
    global_ensemble_results <- perform_global_ensemble_analysis(
      list(probability_matrices = probability_matrices), 
      weights, 
      analysis_type
    )
    
    # Generate globally optimized ensemble matrices
    global_optimized_ensemble_matrices <- generate_global_optimized_ensemble_matrices(
      list(probability_matrices = probability_matrices), 
      weights, 
      analysis_type,
      global_ensemble_results
    )
    
    # Analyze globally optimized ensemble performance
    global_optimized_ensemble_performance <- analyze_optimized_ensemble_performance(global_optimized_ensemble_matrices, analysis_type)
    
    # Perform One-vs-Rest ensemble analysis (properly handles OvR classification)
    ovr_ensemble_results <- perform_ovr_ensemble_analysis(
      list(probability_matrices = probability_matrices), 
      weights, 
      analysis_type
    )
    
    # Generate One-vs-Rest optimized ensemble matrices
    ovr_optimized_result <- generate_ovr_optimized_ensemble_matrices(
      list(probability_matrices = probability_matrices), 
      weights, 
      analysis_type,
      ovr_ensemble_results
    )
    
    # Analyze One-vs-Rest ensemble multiclass performance
    ovr_ensemble_multiclass_performance <- analyze_ovr_ensemble_multiclass_performance(ovr_optimized_result, analysis_type)
    
    # Store results for this analysis type
    results[[analysis_type]] <- list(
      global_ensemble_results = global_ensemble_results,
      global_optimized_ensemble_matrices = global_optimized_ensemble_matrices,
      global_optimized_ensemble_performance = global_optimized_ensemble_performance,
      global_ensemble_weights_used = global_optimized_ensemble_matrices$weights_used,
      
      ovr_ensemble_results = ovr_ensemble_results,
      ovr_optimized_ensemble_matrices = ovr_optimized_result$matrices,
      ovr_ensemble_multiclass_performance = ovr_ensemble_multiclass_performance,
      ovr_ensemble_weights_used = ovr_optimized_result$weights_used
    )
  }
  
  results
}
```


```{r}
# Run ensemble analysis for both CV and LOSO
ensemble_results_1 <- run_ensemble_analysis_for_both_types(probability_matrices_1, generate_weights(0.1))
ensemble_results_2 <- run_ensemble_analysis_for_both_types(probability_matrices_2, generate_weights(0.1))

```
```{r}
#' Calculate and display mean kappa across folds for all ensemble methods
#' @param results Analysis results containing all ensemble performance metrics
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with mean kappa for each ensemble method
compare_ensemble_performance <- function(results, type = "cv") {
  
  folds <- names(results$probability_matrices$svm[[type]])
  performance_summary <- list()
  
  # Individual model performance
  for (model_name in c("svm", "xgboost", "neural_net")) {
    model_kappas <- numeric(length(folds))
    
    for (i in seq_along(folds)) {
      fold <- folds[i]
      optimized_matrix <- results$probability_matrices[[model_name]][[type]][[fold]]
      
      # Extract true labels and remove from probability matrix
      truth <- make.names(optimized_matrix$y)
      prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
      
      # Get predictions
      preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
      
      # Clean class labels
      truth <- gsub("Class.", "", truth)
      preds <- gsub("Class.", "", preds)
      truth <- modify_classes(truth)
      preds <- modify_classes(preds)
      
      # Ensure all classes are represented
      all_classes <- unique(c(truth, preds))
      truth <- factor(truth, levels = all_classes)
      preds <- factor(preds, levels = all_classes)
      
      # Calculate confusion matrix and metrics
      cm <- caret::confusionMatrix(preds, truth)
      model_kappas[i] <- cm$overall["Kappa"]
    }
    
    mean_kappa <- mean(model_kappas, na.rm = TRUE)
    sd_kappa <- sd(model_kappas, na.rm = TRUE)
    
    performance_summary[[toupper(model_name)]] <- list(
      mean_kappa = mean_kappa,
      sd_kappa = sd_kappa,
      fold_kappas = model_kappas
    )
  }
  
  # Ensemble method performance
  ensemble_methods <- list(
    "OvR_Ensemble" = results$ovr_ensemble_multiclass_performance,
    "Global_Optimized" = results$global_optimized_ensemble_performance
  )
  
  for (method_name in names(ensemble_methods)) {
    method_performance <- ensemble_methods[[method_name]]
    method_kappas <- numeric(length(folds))
    
    for (i in seq_along(folds)) {
      fold <- folds[i]
      if (fold %in% names(method_performance)) {
        cm <- method_performance[[fold]]
        method_kappas[i] <- cm$overall["Kappa"]
      }
    }
    
    mean_kappa <- mean(method_kappas, na.rm = TRUE)
    sd_kappa <- sd(method_kappas, na.rm = TRUE)
    
    performance_summary[[method_name]] <- list(
      mean_kappa = mean_kappa,
      sd_kappa = sd_kappa,
      fold_kappas = method_kappas
    )
  }
  
  # Create summary data frame
  summary_df <- data.frame(
    Method = names(performance_summary),
    Mean_Kappa = sapply(performance_summary, function(x) x$mean_kappa),
    SD_Kappa = sapply(performance_summary, function(x) x$sd_kappa),
    stringsAsFactors = FALSE
  )
  
  # Sort by mean kappa (descending)
  summary_df <- summary_df[order(summary_df$Mean_Kappa, decreasing = TRUE), ]
  
  print(summary_df)
  
  # Detailed fold-by-fold comparison
  for (fold in folds) {
    
    # Individual models
    for (model_name in c("svm", "xgboost", "neural_net")) {
      optimized_matrix <- results$probability_matrices[[model_name]][[type]][[fold]]
      
      # Extract true labels and remove from probability matrix
      truth <- make.names(optimized_matrix$y)
      prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
      
      # Get predictions
      preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
      
      # Clean class labels
      truth <- gsub("Class.", "", truth)
      preds <- gsub("Class.", "", preds)
      truth <- modify_classes(truth)
      preds <- modify_classes(preds)
      
      # Ensure all classes are represented
      all_classes <- unique(c(truth, preds))
      truth <- factor(truth, levels = all_classes)
      preds <- factor(preds, levels = all_classes)
      
      # Calculate confusion matrix and metrics
      cm <- caret::confusionMatrix(preds, truth)
    }
    
  }
  
  return(summary_df)
}

#' Compare ensemble performance for both CV and LOSO
#' @param results Analysis results containing all ensemble performance metrics
#' @return List of performance comparisons for both CV and LOSO
compare_ensemble_performance_for_both_types <- function(results) {
  
  performance_comparisons <- list()
  
  for (analysis_type in c("cv", "loso")) {
    
    # Check if we have results for this analysis type
    if (!analysis_type %in% names(results)) {
      next
    }
    
    # Create results list for performance comparison
    comparison_results <- list(
      probability_matrices = results$probability_matrices,
      ovr_ensemble_multiclass_performance = results[[analysis_type]]$ovr_ensemble_multiclass_performance,
      global_optimized_ensemble_performance = results[[analysis_type]]$global_optimized_ensemble_performance
    )
    
    # Compare all ensemble methods and display mean kappa across folds
    performance_comparison <- compare_ensemble_performance(comparison_results, analysis_type)
    performance_comparisons[[analysis_type]] <- performance_comparison
  }
  
  performance_comparisons
}

# Compare ensemble performance for both CV and LOSO
performance_comparisons <- compare_ensemble_performance_for_both_types(
    list(probability_matrices = probability_matrices_1, cv = ensemble_results_1$cv, loso = ensemble_results_1$loso)
  )
```
```{r}
performance_comparisons_2 <- compare_ensemble_performance_for_both_types(
    list(probability_matrices = probability_matrices_2, cv = ensemble_results_2$cv, loso = ensemble_results_2$loso)
  )
```

```{r}
#' Evaluate nested CV kappa with rejection for a single probability matrix
#' @param prob_matrix Probability matrix with class probabilities and true labels
#' @param fold_name Name of the fold being analyzed
#' @param model_name Name of the model being analyzed
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with rejection analysis results
evaluate_single_matrix_with_rejection <- function(prob_matrix, fold_name, model_name, type) {
  # Extract true labels and remove from probability matrix
  truth <- prob_matrix$y
  prob_matrix_clean <- prob_matrix[, !colnames(prob_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
  
  # Clean class labels
  truth <- gsub("Class. ", "", truth)
  truth <- modify_classes(truth)
  
  # Get predictions (class with highest probability)
  pred_indices <- apply(prob_matrix_clean, 1, which.max)
  preds <- colnames(prob_matrix_clean)[pred_indices]
  preds <- gsub("Class. ", "", preds)
  preds <- modify_classes(preds)
  
  # Get max probabilities for each sample
  max_probs <- apply(prob_matrix_clean, 1, max)
  
  # Ensure all classes are represented
  all_classes <- unique(c(truth, preds))
  truth <- factor(truth, levels = all_classes)
  preds <- factor(preds, levels = all_classes)
  
  # Test probability cutoffs 
  prob_cutoffs <- seq(0.00, 1.00, by = 0.01)
  all_results <- data.frame()
  
  for (cutoff in prob_cutoffs) {
    # Identify samples to reject (max probability below cutoff)
    rejected_indices <- which(max_probs < cutoff)
    accepted_indices <- which(max_probs >= cutoff)
    
    if (length(accepted_indices) == 0) {
      # If all samples are rejected, skip this cutoff
      next
    }
    
    # Calculate accuracy for rejected samples (if any)
    rejected_accuracy <- NA
    if (length(rejected_indices) > 0) {
      rejected_truth <- truth[rejected_indices]
      rejected_preds <- preds[rejected_indices]
      rejected_accuracy <- sum(rejected_truth == rejected_preds) / length(rejected_indices)
    }
    # Only proceed if rejected samples have accuracy < 50% (or if no samples are rejected)
      # Use only accepted samples for kappa calculation
      accepted_truth <- truth[accepted_indices]
      accepted_preds <- preds[accepted_indices]
      
      # Calculate kappa for accepted samples
      res <- caret::confusionMatrix(accepted_preds, accepted_truth)
      kappa <- as.numeric(res$overall["Kappa"])
      accuracy <- as.numeric(res$overall["Accuracy"])
      
      # Store results
      all_results <- rbind(
        all_results,
        data.frame(
          model = model_name,
          type = type,
          fold = fold_name,
          prob_cutoff = cutoff,
          kappa = kappa,
          accuracy = accuracy,
          n_accepted = length(accepted_indices),
          n_rejected = length(rejected_indices),
          perc_rejected = length(rejected_indices) / (length(accepted_indices) + length(rejected_indices)),
          rejected_accuracy = rejected_accuracy,
          total_samples = nrow(prob_matrix),
          stringsAsFactors = FALSE
        )
      )
  }
  
  return(all_results)
}

#' Evaluate rejection analysis for all probability matrices
#' @param probability_matrices List of probability matrices for all models
#' @param ensemble_matrices List of ensemble probability matrices
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with rejection analysis results for all models and ensembles
evaluate_all_matrices_with_rejection <- function(probability_matrices, ensemble_matrices, type = "cv") {
  cat("Performing rejection analysis for all probability matrices...\n")
  
  all_rejection_results <- data.frame()
  
  # Analyze individual models
  cat("  Analyzing individual models...\n")
  for (model_name in names(probability_matrices)) {
    cat(sprintf("    Processing %s...\n", toupper(model_name)))
    
    if (type %in% names(probability_matrices[[model_name]])) {
      fold_matrices <- probability_matrices[[model_name]][[type]]
      
      for (fold_name in names(fold_matrices)) {
        prob_matrix <- fold_matrices[[fold_name]]
        
        if (!is.null(prob_matrix) && nrow(prob_matrix) > 0) {
          rejection_results <- evaluate_single_matrix_with_rejection(
            prob_matrix, fold_name, model_name, type
          )
          all_rejection_results <- rbind(all_rejection_results, rejection_results)
        }
      }
    }
  }
  
  # Analyze ensemble methods
  cat("  Analyzing ensemble methods...\n")
  ensemble_methods <- list(
    "OvR_Ensemble" = ensemble_matrices$ovr_optimized_ensemble_matrices,
    #"Per_Class_Optimized" = ensemble_matrices$optimized_ensemble_matrices,
    "Global_Optimized" = ensemble_matrices$global_optimized_ensemble_matrices
  )
  
  for (ensemble_name in names(ensemble_methods)) {
    cat(sprintf("    Processing %s...\n", ensemble_name))
    
    ensemble_matrices_fold <- ensemble_methods[[ensemble_name]]
    
    for (fold_name in names(ensemble_matrices_fold)) {
      prob_matrix <- ensemble_matrices_fold[[fold_name]]
      
      if (!is.null(prob_matrix) && nrow(prob_matrix) > 0) {
        rejection_results <- evaluate_single_matrix_with_rejection(
          prob_matrix, fold_name, ensemble_name, type
        )
        all_rejection_results <- rbind(all_rejection_results, rejection_results)
      }
    }
  }
  
  return(all_rejection_results)
}

#' Find optimal probability cutoff for each model/ensemble
#' @param rejection_results Data frame with rejection analysis results
#' @param optimization_metric Metric to optimize ("kappa" or "accuracy")
#' @return Data frame with optimal cutoffs for each model/ensemble
find_optimal_cutoffs <- function(rejection_results, optimization_metric = "kappa") {
  cat("Finding optimal probability cutoffs...\n")
  
  # Group by model and fold, then find the cutoff that maximizes the optimization metric
  optimal_cutoffs <- rejection_results %>%
    group_by(model, fold) %>%
    filter(!is.na(!!sym(optimization_metric))) %>%
    filter( (is.na(rejected_accuracy) | rejected_accuracy < 0.5) & (perc_rejected < 0.025) ) %>%
    slice_max(!!sym(optimization_metric), with_ties = FALSE) %>%
    ungroup()
  
  # Calculate summary statistics across folds for each model
  summary_stats <- optimal_cutoffs %>%
    group_by(model) %>%
    summarise(
      mean_cutoff = mean(prob_cutoff, na.rm = TRUE),
      sd_cutoff = sd(prob_cutoff, na.rm = TRUE),
      mean_kappa = mean(kappa, na.rm = TRUE),
      sd_kappa = sd(kappa, na.rm = TRUE),
      mean_accuracy = mean(accuracy, na.rm = TRUE),
      sd_accuracy = sd(accuracy, na.rm = TRUE),
      mean_perc_rejected = mean(perc_rejected, na.rm = TRUE),
      sd_perc_rejected = sd(perc_rejected, na.rm = TRUE),
      n_folds = n(),
      .groups = "drop"
    )
  
  return(list(
    optimal_cutoffs = optimal_cutoffs,
    summary_stats = summary_stats
  ))
}

#' Generate rejection analysis plots
#' @param rejection_results Data frame with rejection analysis results
#' @param type Type of analysis ("cv" or "loso")
generate_rejection_plots <- function(rejection_results, type = "cv") {
  cat("Generating rejection analysis plots...\n")
  
  # Load plotting libraries
  load_library_quietly("ggplot2")
  load_library_quietly("gridExtra")
  
  # Plot 1: Kappa vs Probability Cutoff for each model
  p1 <- ggplot(rejection_results, aes(x = prob_cutoff, y = kappa, color = model)) +
    geom_line(alpha = 0.7) +
    facet_wrap(~model, scales = "free_y") +
    labs(title = sprintf("Kappa vs Probability Cutoff (%s)", toupper(type)),
         x = "Probability Cutoff",
         y = "Kappa") +
    theme_minimal() +
    theme(legend.position = "none")
  
  # Plot 2: Percentage Rejected vs Probability Cutoff
  p2 <- ggplot(rejection_results, aes(x = prob_cutoff, y = perc_rejected * 100, color = model)) +
    geom_line(alpha = 0.7) +
    facet_wrap(~model, scales = "free_y") +
    labs(title = sprintf("Percentage Rejected vs Probability Cutoff (%s)", toupper(type)),
         x = "Probability Cutoff",
         y = "Percentage Rejected (%)") +
    theme_minimal() +
    theme(legend.position = "none")
  
  # Plot 3: Kappa vs Percentage Rejected (trade-off analysis)
  p3 <- ggplot(rejection_results, aes(x = perc_rejected * 100, y = kappa, color = model)) +
    geom_point(alpha = 0.6) +
    facet_wrap(~model, scales = "free") +
    labs(title = sprintf("Kappa vs Percentage Rejected Trade-off (%s)", toupper(type)),
         x = "Percentage Rejected (%)",
         y = "Kappa") +
    theme_minimal() +
    theme(legend.position = "none")
  
  # Plot 4: Accuracy vs Probability Cutoff
  p4 <- ggplot(rejection_results, aes(x = prob_cutoff, y = accuracy, color = model)) +
    geom_line(alpha = 0.7) +
    facet_wrap(~model, scales = "free_y") +
    labs(title = sprintf("Accuracy vs Probability Cutoff (%s)", toupper(type)),
         x = "Probability Cutoff",
         y = "Accuracy") +
    theme_minimal() +
    theme(legend.position = "none")
  
  
  # Create combined plot
  combined_plots <- list(p1, p2, p3, p4)
  return(combined_plots)
}

#' Run complete rejection analysis for both CV and LOSO
#' @param probability_matrices Probability matrices for all models
#' @param ensemble_results Ensemble analysis results
#' @param output_base_dir Base directory for output files
#' @return List of rejection analysis results
run_complete_rejection_analysis <- function(probability_matrices, ensemble_results) {
  cat("Running complete rejection analysis...\n")
  
  rejection_results <- list()
  
  for (analysis_type in c("cv", "loso")) {
    cat(sprintf("\n=== Running rejection analysis for %s ===\n", toupper(analysis_type)))
    
    # Check if we have data for this analysis type
    if (!analysis_type %in% names(ensemble_results)) {
      cat(sprintf("Skipping %s rejection analysis - missing ensemble results\n", toupper(analysis_type)))
      next
    }
    
    # Extract ensemble matrices for this analysis type
    ensemble_matrices <- list(
      ovr_optimized_ensemble_matrices = ensemble_results[[analysis_type]]$ovr_optimized_ensemble_matrices,
      global_optimized_ensemble_matrices = ensemble_results[[analysis_type]]$global_optimized_ensemble_matrices$matrices
    )
    
    # Perform rejection analysis
    rejection_results[[analysis_type]][["all_results"]] <- evaluate_all_matrices_with_rejection(
      probability_matrices, ensemble_matrices, analysis_type
    )
    
    # Find optimal cutoffs
    rejection_results[[analysis_type]][["optimal_results"]] <- find_optimal_cutoffs(rejection_results[[analysis_type]][["all_results"]], "kappa")
    
    # Generate plots
    rejection_results[[analysis_type]][["combined_plots"]] <- generate_rejection_plots(rejection_results[[analysis_type]][["all_results"]], analysis_type)
  }
  
  return(rejection_results)
}

#' Create directory safely
#' @param dir_path Directory path to create
create_directory_safely <- function(dir_path) {
  if (!dir.exists(dir_path)) {
    dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  }
}
```


```{r}
# Define output directory for saving results
rejection_results_1 <- run_complete_rejection_analysis(
    probability_matrices_1, ensemble_results_1
  )

rejection_results_2 <- run_complete_rejection_analysis(
    probability_matrices_2, ensemble_results_2
  )
```

```{r}
rejection_results_2[["cv"]][["optimal_results"]][["summary_stats"]]
rejection_results_2[["loso"]][["optimal_results"]][["summary_stats"]]
```

```{r}
inner_cv_results <- list(
    model_results = model_results,
    best_parameters = best_parameters,
    probability_matrices = probability_matrices,
    ensemble_results = ensemble_results,
    performance_comparisons = performance_comparisons,
    rejection_results = rejection_results
  )

inner_cv_results <- list(
    model_results = model_results,
    best_parameters = best_parameters,
    probability_matrices = probability_matrices_2,
    ensemble_results = ensemble_results_2,
    performance_comparisons = performance_comparisons_2,
    rejection_results = rejection_results_2
  )
```

