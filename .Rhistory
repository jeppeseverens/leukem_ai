all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Calculate confusion matrix and metrics
cm <- caret::confusionMatrix(preds, truth)
performance_results[[fold_name]] <- cm
}
# Combine all fold results
performance_results
}
# =============================================================================
# Matrix Alignment Functions
# =============================================================================
#' Align probability matrices from different models for ensemble analysis
#' @param prob_matrices List of probability matrices from different models
#' @param fold_name Name of the fold being processed
#' @param type Type of analysis ("cv" or "loso")
#' @return List of aligned probability matrices
align_probability_matrices <- function(prob_matrices, fold_name, type) {
# Extract matrices for this fold
svm_matrix <- prob_matrices$svm[[type]][[fold_name]]
xgb_matrix <- prob_matrices$xgboost[[type]][[fold_name]]
nn_matrix <- prob_matrices$neural_net[[type]][[fold_name]]
# Check if all matrices exist
if (is.null(svm_matrix) || is.null(xgb_matrix) || is.null(nn_matrix)) {
warning(sprintf("Missing probability matrix for fold %s in %s analysis", fold_name, type))
return(NULL)
}
# Extract true labels
truth_svm <- make.names(svm_matrix$y)
truth_xgb <- make.names(xgb_matrix$y)
truth_nn <- make.names(nn_matrix$y)
# Remove non-probability columns from matrices
svm_matrix <- svm_matrix[, !colnames(svm_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
xgb_matrix <- xgb_matrix[, !colnames(xgb_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
nn_matrix <- nn_matrix[, !colnames(nn_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Get all unique class names across all models
all_classes <- unique(c(
colnames(svm_matrix),
colnames(xgb_matrix),
colnames(nn_matrix)
))
# Get the minimum number of samples across all models
min_samples <- min(nrow(svm_matrix), nrow(xgb_matrix), nrow(nn_matrix))
max_samples <- max(nrow(svm_matrix), nrow(xgb_matrix), nrow(nn_matrix))
# Align matrices by ensuring they have the same columns and sample size
aligned_matrices <- list()
for (model_name in c("svm", "xgboost", "neural_net")) {
matrix_data <- switch(model_name,
"svm" = svm_matrix,
"xgboost" = xgb_matrix,
"neural_net" = nn_matrix
)
# Ensure all required columns exist (add 0s for missing classes)
missing_cols <- setdiff(all_classes, colnames(matrix_data))
for (col in missing_cols) {
matrix_data[[col]] <- 0
}
# Reorder columns to match all_classes
matrix_data <- matrix_data[, all_classes, drop = FALSE]
if (nrow(matrix_data) < max_samples) {
cat(sprintf("The probabilties for %s have less samples then max_samples\n", model_name))
}
# Truncate to minimum sample size if necessary
if (nrow(matrix_data) > min_samples) {
matrix_data <- matrix_data[1:min_samples, , drop = FALSE]
}
aligned_matrices[[model_name]] <- matrix_data
}
# Use the truth from SVM as reference (or the one with minimum samples)
reference_truth <- if (length(truth_svm) >= min_samples) {
truth_svm[1:min_samples]
} else if (length(truth_xgb) >= min_samples) {
truth_xgb[1:min_samples]
} else {
truth_nn[1:min_samples]
}
# Add aligned truth to the result
aligned_matrices$truth <- reference_truth
aligned_matrices
}
#' Run ensemble analysis for both CV and LOSO types
#' @param probability_matrices Probability matrices for all models
#' @param weights Weight configurations for ensemble
#' @return List of results for both CV and LOSO
run_ensemble_analysis_for_both_types <- function(probability_matrices, weights) {
cat("Running ensemble analysis for both CV and LOSO...\n")
results <- list()
for (analysis_type in c("cv", "loso")) {
cat(sprintf("\n=== Running %s analysis ===\n", toupper(analysis_type)))
# Check if we have data for this analysis type
if (!all(sapply(probability_matrices, function(x) analysis_type %in% names(x)))) {
cat(sprintf("Skipping %s analysis - missing data\n", toupper(analysis_type)))
next
}
# Perform global ensemble analysis
global_ensemble_results <- perform_global_ensemble_analysis(
list(probability_matrices = probability_matrices),
weights,
analysis_type
)
# Generate globally optimized ensemble matrices
global_optimized_ensemble_matrices <- generate_global_optimized_ensemble_matrices(
list(probability_matrices = probability_matrices),
weights,
analysis_type,
global_ensemble_results
)
# Analyze globally optimized ensemble performance
global_optimized_ensemble_performance <- analyze_optimized_ensemble_performance(global_optimized_ensemble_matrices, analysis_type)
# Perform One-vs-Rest ensemble analysis (properly handles OvR classification)
ovr_ensemble_results <- perform_ovr_ensemble_analysis(
list(probability_matrices = probability_matrices),
weights,
analysis_type
)
# Generate One-vs-Rest optimized ensemble matrices
ovr_optimized_result <- generate_ovr_optimized_ensemble_matrices(
list(probability_matrices = probability_matrices),
weights,
analysis_type,
ovr_ensemble_results
)
# Analyze One-vs-Rest ensemble multiclass performance
ovr_ensemble_multiclass_performance <- analyze_ovr_ensemble_multiclass_performance(ovr_optimized_result, analysis_type)
# Store results for this analysis type
results[[analysis_type]] <- list(
global_ensemble_results = global_ensemble_results,
global_optimized_ensemble_matrices = global_optimized_ensemble_matrices,
global_optimized_ensemble_performance = global_optimized_ensemble_performance,
global_ensemble_weights_used = global_optimized_ensemble_matrices$weights_used,
ovr_ensemble_results = ovr_ensemble_results,
ovr_optimized_ensemble_matrices = ovr_optimized_result$matrices,
ovr_ensemble_multiclass_performance = ovr_ensemble_multiclass_performance,
ovr_ensemble_weights_used = ovr_optimized_result$weights_used
)
}
results
}
# Run ensemble analysis for both CV and LOSO
ensemble_results_1 <- run_ensemble_analysis_for_both_types(probability_matrices_1, generate_weights(0.1))
ensemble_results_2 <- run_ensemble_analysis_for_both_types(probability_matrices_2, generate_weights(0.1))
#' Calculate and display mean kappa across folds for all ensemble methods
#' @param results Analysis results containing all ensemble performance metrics
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with mean kappa for each ensemble method
compare_ensemble_performance <- function(results, type = "cv") {
folds <- names(results$probability_matrices$svm[[type]])
performance_summary <- list()
# Individual model performance
for (model_name in c("svm", "xgboost", "neural_net")) {
model_kappas <- numeric(length(folds))
for (i in seq_along(folds)) {
fold <- folds[i]
optimized_matrix <- results$probability_matrices[[model_name]][[type]][[fold]]
# Extract true labels and remove from probability matrix
truth <- make.names(optimized_matrix$y)
prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Get predictions
preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
# Clean class labels
truth <- gsub("Class.", "", truth)
preds <- gsub("Class.", "", preds)
truth <- modify_classes(truth)
preds <- modify_classes(preds)
# Ensure all classes are represented
all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Calculate confusion matrix and metrics
cm <- caret::confusionMatrix(preds, truth)
model_kappas[i] <- cm$overall["Kappa"]
}
mean_kappa <- mean(model_kappas, na.rm = TRUE)
sd_kappa <- sd(model_kappas, na.rm = TRUE)
performance_summary[[toupper(model_name)]] <- list(
mean_kappa = mean_kappa,
sd_kappa = sd_kappa,
fold_kappas = model_kappas
)
}
# Ensemble method performance
ensemble_methods <- list(
"OvR_Ensemble" = results$ovr_ensemble_multiclass_performance,
"Global_Optimized" = results$global_optimized_ensemble_performance
)
for (method_name in names(ensemble_methods)) {
method_performance <- ensemble_methods[[method_name]]
method_kappas <- numeric(length(folds))
for (i in seq_along(folds)) {
fold <- folds[i]
if (fold %in% names(method_performance)) {
cm <- method_performance[[fold]]
method_kappas[i] <- cm$overall["Kappa"]
}
}
mean_kappa <- mean(method_kappas, na.rm = TRUE)
sd_kappa <- sd(method_kappas, na.rm = TRUE)
performance_summary[[method_name]] <- list(
mean_kappa = mean_kappa,
sd_kappa = sd_kappa,
fold_kappas = method_kappas
)
}
# Create summary data frame
summary_df <- data.frame(
Method = names(performance_summary),
Mean_Kappa = sapply(performance_summary, function(x) x$mean_kappa),
SD_Kappa = sapply(performance_summary, function(x) x$sd_kappa),
stringsAsFactors = FALSE
)
# Sort by mean kappa (descending)
summary_df <- summary_df[order(summary_df$Mean_Kappa, decreasing = TRUE), ]
print(summary_df)
# Detailed fold-by-fold comparison
for (fold in folds) {
# Individual models
for (model_name in c("svm", "xgboost", "neural_net")) {
optimized_matrix <- results$probability_matrices[[model_name]][[type]][[fold]]
# Extract true labels and remove from probability matrix
truth <- make.names(optimized_matrix$y)
prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Get predictions
preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
# Clean class labels
truth <- gsub("Class.", "", truth)
preds <- gsub("Class.", "", preds)
truth <- modify_classes(truth)
preds <- modify_classes(preds)
# Ensure all classes are represented
all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Calculate confusion matrix and metrics
cm <- caret::confusionMatrix(preds, truth)
}
}
return(summary_df)
}
#' Compare ensemble performance for both CV and LOSO
#' @param results Analysis results containing all ensemble performance metrics
#' @return List of performance comparisons for both CV and LOSO
compare_ensemble_performance_for_both_types <- function(results) {
performance_comparisons <- list()
for (analysis_type in c("cv", "loso")) {
# Check if we have results for this analysis type
if (!analysis_type %in% names(results)) {
next
}
# Create results list for performance comparison
comparison_results <- list(
probability_matrices = results$probability_matrices,
ovr_ensemble_multiclass_performance = results[[analysis_type]]$ovr_ensemble_multiclass_performance,
global_optimized_ensemble_performance = results[[analysis_type]]$global_optimized_ensemble_performance
)
# Compare all ensemble methods and display mean kappa across folds
performance_comparison <- compare_ensemble_performance(comparison_results, analysis_type)
performance_comparisons[[analysis_type]] <- performance_comparison
}
performance_comparisons
}
# Compare ensemble performance for both CV and LOSO
performance_comparisons <- compare_ensemble_performance_for_both_types(
list(probability_matrices = probability_matrices_1, cv = ensemble_results_1$cv, loso = ensemble_results_1$loso)
)
performance_comparisons_2 <- compare_ensemble_performance_for_both_types(
list(probability_matrices = probability_matrices_2, cv = ensemble_results_2$cv, loso = ensemble_results_2$loso)
)
#' Evaluate nested CV kappa with rejection for a single probability matrix
#' @param prob_matrix Probability matrix with class probabilities and true labels
#' @param fold_name Name of the fold being analyzed
#' @param model_name Name of the model being analyzed
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with rejection analysis results
evaluate_single_matrix_with_rejection <- function(prob_matrix, fold_name, model_name, type) {
# Extract true labels and remove from probability matrix
truth <- prob_matrix$y
prob_matrix_clean <- prob_matrix[, !colnames(prob_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Clean class labels
truth <- gsub("Class. ", "", truth)
truth <- modify_classes(truth)
# Get predictions (class with highest probability)
pred_indices <- apply(prob_matrix_clean, 1, which.max)
preds <- colnames(prob_matrix_clean)[pred_indices]
preds <- gsub("Class. ", "", preds)
preds <- modify_classes(preds)
# Get max probabilities for each sample
max_probs <- apply(prob_matrix_clean, 1, max)
# Ensure all classes are represented
all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Test probability cutoffs
prob_cutoffs <- seq(0.00, 1.00, by = 0.01)
all_results <- data.frame()
for (cutoff in prob_cutoffs) {
# Identify samples to reject (max probability below cutoff)
rejected_indices <- which(max_probs < cutoff)
accepted_indices <- which(max_probs >= cutoff)
if (length(accepted_indices) == 0) {
# If all samples are rejected, skip this cutoff
next
}
# Calculate accuracy for rejected samples (if any)
rejected_accuracy <- NA
if (length(rejected_indices) > 0) {
rejected_truth <- truth[rejected_indices]
rejected_preds <- preds[rejected_indices]
rejected_accuracy <- sum(rejected_truth == rejected_preds) / length(rejected_indices)
}
# Only proceed if rejected samples have accuracy < 50% (or if no samples are rejected)
# Use only accepted samples for kappa calculation
accepted_truth <- truth[accepted_indices]
accepted_preds <- preds[accepted_indices]
# Calculate kappa for accepted samples
res <- caret::confusionMatrix(accepted_preds, accepted_truth)
kappa <- as.numeric(res$overall["Kappa"])
accuracy <- as.numeric(res$overall["Accuracy"])
# Store results
all_results <- rbind(
all_results,
data.frame(
model = model_name,
type = type,
fold = fold_name,
prob_cutoff = cutoff,
kappa = kappa,
accuracy = accuracy,
n_accepted = length(accepted_indices),
n_rejected = length(rejected_indices),
perc_rejected = length(rejected_indices) / (length(accepted_indices) + length(rejected_indices)),
rejected_accuracy = rejected_accuracy,
total_samples = nrow(prob_matrix),
stringsAsFactors = FALSE
)
)
}
return(all_results)
}
#' Evaluate rejection analysis for all probability matrices
#' @param probability_matrices List of probability matrices for all models
#' @param ensemble_matrices List of ensemble probability matrices
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with rejection analysis results for all models and ensembles
evaluate_all_matrices_with_rejection <- function(probability_matrices, ensemble_matrices, type = "cv") {
cat("Performing rejection analysis for all probability matrices...\n")
all_rejection_results <- data.frame()
# Analyze individual models
cat("  Analyzing individual models...\n")
for (model_name in names(probability_matrices)) {
cat(sprintf("    Processing %s...\n", toupper(model_name)))
if (type %in% names(probability_matrices[[model_name]])) {
fold_matrices <- probability_matrices[[model_name]][[type]]
for (fold_name in names(fold_matrices)) {
prob_matrix <- fold_matrices[[fold_name]]
if (!is.null(prob_matrix) && nrow(prob_matrix) > 0) {
rejection_results <- evaluate_single_matrix_with_rejection(
prob_matrix, fold_name, model_name, type
)
all_rejection_results <- rbind(all_rejection_results, rejection_results)
}
}
}
}
# Analyze ensemble methods
cat("  Analyzing ensemble methods...\n")
ensemble_methods <- list(
"OvR_Ensemble" = ensemble_matrices$ovr_optimized_ensemble_matrices,
#"Per_Class_Optimized" = ensemble_matrices$optimized_ensemble_matrices,
"Global_Optimized" = ensemble_matrices$global_optimized_ensemble_matrices
)
for (ensemble_name in names(ensemble_methods)) {
cat(sprintf("    Processing %s...\n", ensemble_name))
ensemble_matrices_fold <- ensemble_methods[[ensemble_name]]
for (fold_name in names(ensemble_matrices_fold)) {
prob_matrix <- ensemble_matrices_fold[[fold_name]]
if (!is.null(prob_matrix) && nrow(prob_matrix) > 0) {
rejection_results <- evaluate_single_matrix_with_rejection(
prob_matrix, fold_name, ensemble_name, type
)
all_rejection_results <- rbind(all_rejection_results, rejection_results)
}
}
}
return(all_rejection_results)
}
#' Find optimal probability cutoff for each model/ensemble
#' @param rejection_results Data frame with rejection analysis results
#' @param optimization_metric Metric to optimize ("kappa" or "accuracy")
#' @return Data frame with optimal cutoffs for each model/ensemble
find_optimal_cutoffs <- function(rejection_results, optimization_metric = "kappa") {
cat("Finding optimal probability cutoffs...\n")
# Group by model and fold, then find the cutoff that maximizes the optimization metric
optimal_cutoffs <- rejection_results %>%
group_by(model, fold) %>%
filter(!is.na(!!sym(optimization_metric))) %>%
filter( (is.na(rejected_accuracy) | rejected_accuracy < 0.5) & (perc_rejected < 0.025) ) %>%
slice_max(!!sym(optimization_metric), with_ties = FALSE) %>%
ungroup()
# Calculate summary statistics across folds for each model
summary_stats <- optimal_cutoffs %>%
group_by(model) %>%
summarise(
mean_cutoff = mean(prob_cutoff, na.rm = TRUE),
sd_cutoff = sd(prob_cutoff, na.rm = TRUE),
mean_kappa = mean(kappa, na.rm = TRUE),
sd_kappa = sd(kappa, na.rm = TRUE),
mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
mean_perc_rejected = mean(perc_rejected, na.rm = TRUE),
sd_perc_rejected = sd(perc_rejected, na.rm = TRUE),
n_folds = n(),
.groups = "drop"
)
return(list(
optimal_cutoffs = optimal_cutoffs,
summary_stats = summary_stats
))
}
#' Generate rejection analysis plots
#' @param rejection_results Data frame with rejection analysis results
#' @param type Type of analysis ("cv" or "loso")
generate_rejection_plots <- function(rejection_results, type = "cv") {
cat("Generating rejection analysis plots...\n")
# Load plotting libraries
load_library_quietly("ggplot2")
load_library_quietly("gridExtra")
# Plot 1: Kappa vs Probability Cutoff for each model
p1 <- ggplot(rejection_results, aes(x = prob_cutoff, y = kappa, color = model)) +
geom_line(alpha = 0.7) +
facet_wrap(~model, scales = "free_y") +
labs(title = sprintf("Kappa vs Probability Cutoff (%s)", toupper(type)),
x = "Probability Cutoff",
y = "Kappa") +
theme_minimal() +
theme(legend.position = "none")
# Plot 2: Percentage Rejected vs Probability Cutoff
p2 <- ggplot(rejection_results, aes(x = prob_cutoff, y = perc_rejected * 100, color = model)) +
geom_line(alpha = 0.7) +
facet_wrap(~model, scales = "free_y") +
labs(title = sprintf("Percentage Rejected vs Probability Cutoff (%s)", toupper(type)),
x = "Probability Cutoff",
y = "Percentage Rejected (%)") +
theme_minimal() +
theme(legend.position = "none")
# Plot 3: Kappa vs Percentage Rejected (trade-off analysis)
p3 <- ggplot(rejection_results, aes(x = perc_rejected * 100, y = kappa, color = model)) +
geom_point(alpha = 0.6) +
facet_wrap(~model, scales = "free") +
labs(title = sprintf("Kappa vs Percentage Rejected Trade-off (%s)", toupper(type)),
x = "Percentage Rejected (%)",
y = "Kappa") +
theme_minimal() +
theme(legend.position = "none")
# Plot 4: Accuracy vs Probability Cutoff
p4 <- ggplot(rejection_results, aes(x = prob_cutoff, y = accuracy, color = model)) +
geom_line(alpha = 0.7) +
facet_wrap(~model, scales = "free_y") +
labs(title = sprintf("Accuracy vs Probability Cutoff (%s)", toupper(type)),
x = "Probability Cutoff",
y = "Accuracy") +
theme_minimal() +
theme(legend.position = "none")
# Create combined plot
combined_plots <- list(p1, p2, p3, p4)
return(combined_plots)
}
#' Run complete rejection analysis for both CV and LOSO
#' @param probability_matrices Probability matrices for all models
#' @param ensemble_results Ensemble analysis results
#' @param output_base_dir Base directory for output files
#' @return List of rejection analysis results
run_complete_rejection_analysis <- function(probability_matrices, ensemble_results) {
cat("Running complete rejection analysis...\n")
rejection_results <- list()
for (analysis_type in c("cv", "loso")) {
cat(sprintf("\n=== Running rejection analysis for %s ===\n", toupper(analysis_type)))
# Check if we have data for this analysis type
if (!analysis_type %in% names(ensemble_results)) {
cat(sprintf("Skipping %s rejection analysis - missing ensemble results\n", toupper(analysis_type)))
next
}
# Extract ensemble matrices for this analysis type
ensemble_matrices <- list(
ovr_optimized_ensemble_matrices = ensemble_results[[analysis_type]]$ovr_optimized_ensemble_matrices,
global_optimized_ensemble_matrices = ensemble_results[[analysis_type]]$global_optimized_ensemble_matrices$matrices
)
# Perform rejection analysis
rejection_results[[analysis_type]][["all_results"]] <- evaluate_all_matrices_with_rejection(
probability_matrices, ensemble_matrices, analysis_type
)
# Find optimal cutoffs
rejection_results[[analysis_type]][["optimal_results"]] <- find_optimal_cutoffs(rejection_results[[analysis_type]][["all_results"]], "kappa")
# Generate plots
rejection_results[[analysis_type]][["combined_plots"]] <- generate_rejection_plots(rejection_results[[analysis_type]][["all_results"]], analysis_type)
}
return(rejection_results)
}
#' Create directory safely
#' @param dir_path Directory path to create
create_directory_safely <- function(dir_path) {
if (!dir.exists(dir_path)) {
dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
}
}
# Define output directory for saving results
rejection_results_1 <- run_complete_rejection_analysis(
probability_matrices_1, ensemble_results_1
)
rejection_results_2 <- run_complete_rejection_analysis(
probability_matrices_2, ensemble_results_2
)
