)
# Generate globally optimized ensemble matrices
global_optimized_ensemble_matrices <- generate_global_optimized_ensemble_matrices(
list(probability_matrices = probability_matrices),
weights,
analysis_type,
global_ensemble_results
)
# Analyze globally optimized ensemble performance
global_optimized_ensemble_performance <- analyze_optimized_ensemble_performance(global_optimized_ensemble_matrices, analysis_type)
# Perform One-vs-Rest ensemble analysis (properly handles OvR classification)
ovr_ensemble_results <- perform_ovr_ensemble_analysis(
list(probability_matrices = probability_matrices),
weights,
analysis_type
)
# Generate One-vs-Rest optimized ensemble matrices
ovr_optimized_result <- generate_ovr_optimized_ensemble_matrices(
list(probability_matrices = probability_matrices),
weights,
analysis_type,
ovr_ensemble_results
)
# Analyze One-vs-Rest ensemble multiclass performance
ovr_ensemble_multiclass_performance <- analyze_ovr_ensemble_multiclass_performance(ovr_optimized_result, analysis_type)
# Store results for this analysis type
results[[analysis_type]] <- list(
global_ensemble_results = global_ensemble_results,
global_optimized_ensemble_matrices = global_optimized_ensemble_matrices,
global_optimized_ensemble_performance = global_optimized_ensemble_performance,
global_ensemble_weights_used = global_optimized_ensemble_matrices$weights_used,
ovr_ensemble_results = ovr_ensemble_results,
ovr_optimized_ensemble_matrices = ovr_optimized_result$matrices,
ovr_ensemble_multiclass_performance = ovr_ensemble_multiclass_performance,
ovr_ensemble_weights_used = ovr_optimized_result$weights_used
)
}
results
}
# Run ensemble analysis for both CV and LOSO
#ensemble_results_1 <- run_ensemble_analysis_for_both_types(probability_matrices_1, generate_weights(0.1))
ensemble_results_2 <- run_ensemble_analysis_for_both_types(probability_matrices_2, generate_weights(0.1))
weights_dir <- "../inner_cv_best_params_n10/ensemble_weights_20aug"
save_ensemble_weights(ensemble_results_2, weights_dir)
#' Calculate and display mean kappa across folds for all ensemble methods
#' @param results Analysis results containing all ensemble performance metrics
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with mean kappa for each ensemble method
compare_ensemble_performance <- function(results, type = "cv") {
folds <- names(results$probability_matrices$svm[[type]])
performance_summary <- list()
# Individual model performance
for (model_name in c("svm", "xgboost", "neural_net")) {
model_kappas <- numeric(length(folds))
for (i in seq_along(folds)) {
fold <- folds[i]
optimized_matrix <- results$probability_matrices[[model_name]][[type]][[fold]]
# Extract true labels and remove from probability matrix
truth <- make.names(optimized_matrix$y)
prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Get predictions
preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
# Clean class labels
truth <- gsub("Class.", "", truth)
preds <- gsub("Class.", "", preds)
truth <- modify_classes(truth)
preds <- modify_classes(preds)
# Ensure all classes are represented
all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Calculate confusion matrix and metrics
cm <- caret::confusionMatrix(preds, truth)
model_kappas[i] <- cm$overall["Kappa"]
}
mean_kappa <- mean(model_kappas, na.rm = TRUE)
sd_kappa <- sd(model_kappas, na.rm = TRUE)
performance_summary[[toupper(model_name)]] <- list(
mean_kappa = mean_kappa,
sd_kappa = sd_kappa,
fold_kappas = model_kappas
)
}
# Ensemble method performance
ensemble_methods <- list(
"OvR_Ensemble" = results$ovr_ensemble_multiclass_performance,
"Global_Optimized" = results$global_optimized_ensemble_performance
)
for (method_name in names(ensemble_methods)) {
method_performance <- ensemble_methods[[method_name]]
method_kappas <- numeric(length(folds))
for (i in seq_along(folds)) {
fold <- folds[i]
if (fold %in% names(method_performance)) {
cm <- method_performance[[fold]]
method_kappas[i] <- cm$overall["Kappa"]
}
}
mean_kappa <- mean(method_kappas, na.rm = TRUE)
sd_kappa <- sd(method_kappas, na.rm = TRUE)
performance_summary[[method_name]] <- list(
mean_kappa = mean_kappa,
sd_kappa = sd_kappa,
fold_kappas = method_kappas
)
}
# Create summary data frame
summary_df <- data.frame(
Method = names(performance_summary),
Mean_Kappa = sapply(performance_summary, function(x) x$mean_kappa),
SD_Kappa = sapply(performance_summary, function(x) x$sd_kappa),
stringsAsFactors = FALSE
)
# Sort by mean kappa (descending)
summary_df <- summary_df[order(summary_df$Mean_Kappa, decreasing = TRUE), ]
print(summary_df)
# Detailed fold-by-fold comparison
for (fold in folds) {
# Individual models
for (model_name in c("svm", "xgboost", "neural_net")) {
optimized_matrix <- results$probability_matrices[[model_name]][[type]][[fold]]
# Extract true labels and remove from probability matrix
truth <- make.names(optimized_matrix$y)
prob_matrix <- optimized_matrix[, !colnames(optimized_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Get predictions
preds <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
# Clean class labels
truth <- gsub("Class.", "", truth)
preds <- gsub("Class.", "", preds)
truth <- modify_classes(truth)
preds <- modify_classes(preds)
# Ensure all classes are represented
all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Calculate confusion matrix and metrics
cm <- caret::confusionMatrix(preds, truth)
}
}
return(summary_df)
}
#' Compare ensemble performance for both CV and LOSO
#' @param results Analysis results containing all ensemble performance metrics
#' @return List of performance comparisons for both CV and LOSO
compare_ensemble_performance_for_both_types <- function(results) {
performance_comparisons <- list()
for (analysis_type in c("cv", "loso")) {
# Check if we have results for this analysis type
if (!analysis_type %in% names(results)) {
next
}
# Create results list for performance comparison
comparison_results <- list(
probability_matrices = results$probability_matrices,
ovr_ensemble_multiclass_performance = results[[analysis_type]]$ovr_ensemble_multiclass_performance,
global_optimized_ensemble_performance = results[[analysis_type]]$global_optimized_ensemble_performance
)
# Compare all ensemble methods and display mean kappa across folds
performance_comparison <- compare_ensemble_performance(comparison_results, analysis_type)
performance_comparisons[[analysis_type]] <- performance_comparison
}
performance_comparisons
}
# # Compare ensemble performance for both CV and LOSO
# performance_comparisons <- compare_ensemble_performance_for_both_types(
#     list(probability_matrices = probability_matrices_1, cv = ensemble_results_1$cv, loso = ensemble_results_1$loso)
#   )
performance_comparisons_2 <- compare_ensemble_performance_for_both_types(
list(probability_matrices = probability_matrices_2, cv = ensemble_results_2$cv, loso = ensemble_results_2$loso)
)
#' Evaluate nested CV kappa with rejection for a single probability matrix
#' @param prob_matrix Probability matrix with class probabilities and true labels
#' @param fold_name Name of the fold being analyzed
#' @param model_name Name of the model being analyzed
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with rejection analysis results
evaluate_single_matrix_with_rejection <- function(prob_matrix, fold_name, model_name, type) {
# Extract true labels and remove from probability matrix
truth <- prob_matrix$y
prob_matrix_clean <- prob_matrix[, !colnames(prob_matrix) %in% c("y", "inner_fold", "outer_fold", "indices", "study"), drop = FALSE]
# Clean class labels
truth <- gsub("Class. ", "", truth)
truth <- modify_classes(truth)
# Get predictions (class with highest probability)
pred_indices <- apply(prob_matrix_clean, 1, which.max)
preds <- colnames(prob_matrix_clean)[pred_indices]
preds <- gsub("Class. ", "", preds)
preds <- modify_classes(preds)
# Get max probabilities for each sample
max_probs <- apply(prob_matrix_clean, 1, max)
# Ensure all classes are represented
all_classes <- unique(c(truth, preds))
truth <- factor(truth, levels = all_classes)
preds <- factor(preds, levels = all_classes)
# Test probability cutoffs
prob_cutoffs <- seq(0.00, 1.00, by = 0.01)
all_results <- data.frame()
for (cutoff in prob_cutoffs) {
# Identify samples to reject (max probability below cutoff)
rejected_indices <- which(max_probs < cutoff)
accepted_indices <- which(max_probs >= cutoff)
if (length(accepted_indices) == 0) {
# If all samples are rejected, skip this cutoff
next
}
# Calculate accuracy for rejected samples (if any)
rejected_accuracy <- NA
if (length(rejected_indices) > 0) {
rejected_truth <- truth[rejected_indices]
rejected_preds <- preds[rejected_indices]
rejected_accuracy <- sum(rejected_truth == rejected_preds) / length(rejected_indices)
}
# Only proceed if rejected samples have accuracy < 50% (or if no samples are rejected)
# Use only accepted samples for kappa calculation
accepted_truth <- truth[accepted_indices]
accepted_preds <- preds[accepted_indices]
# Calculate kappa for accepted samples
res <- caret::confusionMatrix(accepted_preds, accepted_truth)
kappa <- as.numeric(res$overall["Kappa"])
accuracy <- as.numeric(res$overall["Accuracy"])
# Store results
all_results <- rbind(
all_results,
data.frame(
model = model_name,
type = type,
fold = fold_name,
prob_cutoff = cutoff,
kappa = kappa,
accuracy = accuracy,
n_accepted = length(accepted_indices),
n_rejected = length(rejected_indices),
perc_rejected = length(rejected_indices) / (length(accepted_indices) + length(rejected_indices)),
rejected_accuracy = rejected_accuracy,
total_samples = nrow(prob_matrix),
stringsAsFactors = FALSE
)
)
}
return(all_results)
}
#' Evaluate rejection analysis for all probability matrices
#' @param probability_matrices List of probability matrices for all models
#' @param ensemble_matrices List of ensemble probability matrices
#' @param type Type of analysis ("cv" or "loso")
#' @return Data frame with rejection analysis results for all models and ensembles
evaluate_all_matrices_with_rejection <- function(probability_matrices, ensemble_matrices, type = "cv") {
cat("Performing rejection analysis for all probability matrices...\n")
all_rejection_results <- data.frame()
# Analyze individual models
cat("  Analyzing individual models...\n")
for (model_name in names(probability_matrices)) {
cat(sprintf("    Processing %s...\n", toupper(model_name)))
if (type %in% names(probability_matrices[[model_name]])) {
fold_matrices <- probability_matrices[[model_name]][[type]]
for (fold_name in names(fold_matrices)) {
prob_matrix <- fold_matrices[[fold_name]]
if (!is.null(prob_matrix) && nrow(prob_matrix) > 0) {
rejection_results <- evaluate_single_matrix_with_rejection(
prob_matrix, fold_name, model_name, type
)
all_rejection_results <- rbind(all_rejection_results, rejection_results)
}
}
}
}
# Analyze ensemble methods
cat("  Analyzing ensemble methods...\n")
ensemble_methods <- list(
"OvR_Ensemble" = ensemble_matrices$ovr_optimized_ensemble_matrices,
#"Per_Class_Optimized" = ensemble_matrices$optimized_ensemble_matrices,
"Global_Optimized" = ensemble_matrices$global_optimized_ensemble_matrices
)
for (ensemble_name in names(ensemble_methods)) {
cat(sprintf("    Processing %s...\n", ensemble_name))
ensemble_matrices_fold <- ensemble_methods[[ensemble_name]]
for (fold_name in names(ensemble_matrices_fold)) {
prob_matrix <- ensemble_matrices_fold[[fold_name]]
if (!is.null(prob_matrix) && nrow(prob_matrix) > 0) {
rejection_results <- evaluate_single_matrix_with_rejection(
prob_matrix, fold_name, ensemble_name, type
)
all_rejection_results <- rbind(all_rejection_results, rejection_results)
}
}
}
return(all_rejection_results)
}
#' Find optimal probability cutoff for each model/ensemble
#' @param rejection_results Data frame with rejection analysis results
#' @param optimization_metric Metric to optimize ("kappa" or "accuracy")
#' @return Data frame with optimal cutoffs for each model/ensemble
find_optimal_cutoffs <- function(rejection_results, optimization_metric = "kappa") {
cat("Finding optimal probability cutoffs...\n")
# Group by model and fold, then find the cutoff that maximizes the optimization metric
optimal_cutoffs <- rejection_results %>%
group_by(model, fold) %>%
filter(!is.na(!!sym(optimization_metric))) %>%
filter( (is.na(rejected_accuracy) | rejected_accuracy < 0.5) & (perc_rejected < 0.025) ) %>%
slice_max(!!sym(optimization_metric), with_ties = FALSE) %>%
ungroup()
# Calculate summary statistics across folds for each model
summary_stats <- optimal_cutoffs %>%
group_by(model) %>%
summarise(
mean_cutoff = mean(prob_cutoff, na.rm = TRUE),
sd_cutoff = sd(prob_cutoff, na.rm = TRUE),
mean_kappa = mean(kappa, na.rm = TRUE),
sd_kappa = sd(kappa, na.rm = TRUE),
mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
mean_perc_rejected = mean(perc_rejected, na.rm = TRUE),
sd_perc_rejected = sd(perc_rejected, na.rm = TRUE),
n_folds = n(),
.groups = "drop"
) %>%
arrange(desc(mean_kappa))
return(list(
optimal_cutoffs = optimal_cutoffs,
summary_stats = summary_stats
))
}
#' Generate rejection analysis plots
#' @param rejection_results Data frame with rejection analysis results
#' @param type Type of analysis ("cv" or "loso")
generate_rejection_plots <- function(rejection_results, type = "cv") {
cat("Generating rejection analysis plots...\n")
# Load plotting libraries
load_library_quietly("ggplot2")
load_library_quietly("gridExtra")
# Plot 1: Kappa vs Probability Cutoff for each model
p1 <- ggplot(rejection_results, aes(x = prob_cutoff, y = kappa, color = model)) +
geom_line(alpha = 0.7) +
facet_wrap(~model, scales = "free_y") +
labs(title = sprintf("Kappa vs Probability Cutoff (%s)", toupper(type)),
x = "Probability Cutoff",
y = "Kappa") +
theme_minimal() +
theme(legend.position = "none")
# Plot 2: Percentage Rejected vs Probability Cutoff
p2 <- ggplot(rejection_results, aes(x = prob_cutoff, y = perc_rejected * 100, color = model)) +
geom_line(alpha = 0.7) +
facet_wrap(~model, scales = "free_y") +
labs(title = sprintf("Percentage Rejected vs Probability Cutoff (%s)", toupper(type)),
x = "Probability Cutoff",
y = "Percentage Rejected (%)") +
theme_minimal() +
theme(legend.position = "none")
# Plot 3: Kappa vs Percentage Rejected (trade-off analysis)
p3 <- ggplot(rejection_results, aes(x = perc_rejected * 100, y = kappa, color = model)) +
geom_point(alpha = 0.6) +
facet_wrap(~model, scales = "free") +
labs(title = sprintf("Kappa vs Percentage Rejected Trade-off (%s)", toupper(type)),
x = "Percentage Rejected (%)",
y = "Kappa") +
theme_minimal() +
theme(legend.position = "none")
# Plot 4: Accuracy vs Probability Cutoff
p4 <- ggplot(rejection_results, aes(x = prob_cutoff, y = accuracy, color = model)) +
geom_line(alpha = 0.7) +
facet_wrap(~model, scales = "free_y") +
labs(title = sprintf("Accuracy vs Probability Cutoff (%s)", toupper(type)),
x = "Probability Cutoff",
y = "Accuracy") +
theme_minimal() +
theme(legend.position = "none")
# Create combined plot
combined_plots <- list(p1, p2, p3, p4)
return(combined_plots)
}
#' Run complete rejection analysis for both CV and LOSO
#' @param probability_matrices Probability matrices for all models
#' @param ensemble_results Ensemble analysis results
#' @param output_base_dir Base directory for output files
#' @return List of rejection analysis results
run_complete_rejection_analysis <- function(probability_matrices, ensemble_results) {
cat("Running complete rejection analysis...\n")
rejection_results <- list()
for (analysis_type in c("cv", "loso")) {
cat(sprintf("\n=== Running rejection analysis for %s ===\n", toupper(analysis_type)))
# Check if we have data for this analysis type
if (!analysis_type %in% names(ensemble_results)) {
cat(sprintf("Skipping %s rejection analysis - missing ensemble results\n", toupper(analysis_type)))
next
}
# Extract ensemble matrices for this analysis type
ensemble_matrices <- list(
ovr_optimized_ensemble_matrices = ensemble_results[[analysis_type]]$ovr_optimized_ensemble_matrices,
global_optimized_ensemble_matrices = ensemble_results[[analysis_type]]$global_optimized_ensemble_matrices$matrices
)
# Perform rejection analysis
rejection_results[[analysis_type]][["all_results"]] <- evaluate_all_matrices_with_rejection(
probability_matrices, ensemble_matrices, analysis_type
)
# Find optimal cutoffs
rejection_results[[analysis_type]][["optimal_results"]] <- find_optimal_cutoffs(rejection_results[[analysis_type]][["all_results"]], "kappa")
# Generate plots
rejection_results[[analysis_type]][["combined_plots"]] <- generate_rejection_plots(rejection_results[[analysis_type]][["all_results"]], analysis_type)
}
return(rejection_results)
}
# Define output directory for saving results
# rejection_results_1 <- run_complete_rejection_analysis(
#     probability_matrices_1, ensemble_results_1
#   )
rejection_results_2 <- run_complete_rejection_analysis(
probability_matrices_2, ensemble_results_2
)
rejection_results_2[["cv"]][["optimal_results"]][["summary_stats"]]
rejection_results_2[["loso"]][["optimal_results"]][["summary_stats"]]
# Extract the optimal cutoffs dataframes
cv_cutoffs <- rejection_results_2[["cv"]][["optimal_results"]][["optimal_cutoffs"]]
loso_cutoffs <- rejection_results_2[["loso"]][["optimal_results"]][["optimal_cutoffs"]]
# Add source column to each dataframe
cv_cutoffs$source <- "cv"
loso_cutoffs$source <- "loso"
cutoff_dir <- "../inner_cv_best_params_n10/cutoffs_20aug"
dir.create(cutoff_dir)
# Bind the dataframes together
combined_cutoffs <- rbind(cv_cutoffs, loso_cutoffs)
write.csv(combined_cutoffs, "../inner_cv_best_params_n10/cutoffs_20aug/cutoffs.csv")
table(meta$Studies)
table(meta$Studies)
label_mapping
label_mapping$Label
table(leukemia_subtypes)
table(leukemia_subtypes, meta$Studies)
# Load leukemia subtype data
leukemia_subtypes <- read.csv("../data/rgas_20aug25.csv")$ICC_Subtype
# Load study metadata
meta <- read.csv("../data/meta_20aug25.csv")
table(leukemia_subtypes, meta$Studies)
addm\ table(leukemia_subtypes, meta$Studies))
addmargins() table(leukemia_subtypes, meta$Studies))
addmargins(table(leukemia_subtypes, meta$Studies))
write.csv("/Users/jsevere2/temporary/table_subtypes.csv", addmargins(table(leukemia_subtypes, meta$Studies)))
write.csv(file = "/Users/jsevere2/temporary/table_subtypes.csv", addmargins(table(leukemia_subtypes, meta$Studies)))
table(filtered_leukemia_subtypes)
View(probability_matrices_2)
probability_matrices_2[["svm"]][["cv"]][["0"]]
test <- probability_matrices_2[["svm"]][["cv"]][["0"]]
View(test)
test <- test[sample(1:1000, size = 10),]
test[,c(1:6)] <- round(test[,c(1:6)], 2)
round(0.004786645, 2)
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
# Set working directory
setwd("~/Documents/AML_PhD/leukem_ai")
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
View(OUTER_MODEL_CONFIGS)
View(outer_cv_results)
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
outer_cv_results[["ensemble_weights_used"]][["cv"]][["global_weights"]]
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
View(outer_cv_results)
outer_cv_results[["per_class_summaries"]][["cv"]]$Model
outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized")
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized")
Global_Optimized_per_class
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(-N_Folds)
Global_Optimized_per_class
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(-N_Folds, -Model)
Global_Optimized_per_class
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1)
Global_Optimized_per_class
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1)
Global_Optimized_per_class
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1)
Global_Optimized_per_class
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1)
Global_Optimized_per_class
outer_cv_results[["rejection_per_class_summaries"]][["cv"]]%>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1)
outer_cv_results[["rejection_per_class_summaries"]][["cv"]]%>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1) %>% rename(Mean_F1_with_rej = Mean_F1)
left_join(Global_Optimized_per_class,
outer_cv_results[["rejection_per_class_summaries"]][["cv"]]%>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1) %>% rename(Mean_F1_with_rej = Mean_F1)
)
Global_Optimized_per_class <- left_join(Global_Optimized_per_class,
outer_cv_results[["rejection_per_class_summaries"]][["cv"]]%>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1) %>% rename(Mean_F1_with_rej = Mean_F1)
)
Global_Optimized_per_class
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
outer_cv_results[["combined_confusion_matrices"]][["cv"]]
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
cms <- lapply(outer_cv_results[["detailed_performance"]][["cv"]][["Global_Optimized"]], function(x){
x[["confusion_matrix"]][["table"]]
})
View(cms)
cms[["0"]]
cms[["1"]]
cms[[1]] + cms[[2]]
Global_Optimized_total_cm <- Reduce(`+`, cms)
Global_Optimized_total_cm
source("~/Documents/AML_PhD/leukem_ai/R/analyse_outer_cv.r")
Global_Optimized_per_class <- outer_cv_results[["per_class_summaries"]][["cv"]] %>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1)
Global_Optimized_per_class <- left_join(Global_Optimized_per_class,
outer_cv_results[["rejection_per_class_summaries"]][["cv"]]%>% filter(Model == "Global_Optimized") %>% select(Class, Mean_F1) %>% rename(Mean_F1_with_rej = Mean_F1)
)
Global_Optimized_per_class
cms <- lapply(outer_cv_results[["detailed_performance"]][["cv"]][["Global_Optimized"]], function(x){
x[["confusion_matrix"]][["table"]]
})
Global_Optimized_total_cm <- Reduce(`+`, cms)
Global_Optimized_total_cm
setwd("~/Documents/AML_PhD/leukem_ai")
write.csv(Global_Optimized_total_cm, "~/Documents/AML_PhD/leukem_ai/inner_cv_best_params_n10/combined_cm.csv")
