Article title: 
Jeppe F Severens1,2,3, E Onur Karakaslar1,2,3, … Marcel JT Reinders1,2,3, Marieke Griffioen7, Erik B van den Akker1,2,3

1Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, The Netherlands
2Pattern Recognition & Bioinformatics, Delft University of Technology, Delft, The Netherlands
3Leiden Center for Computational Oncology, Leiden University Medical Center, Leiden, The Netherlands
4Laboratory of Hematology, Department of Laboratory Medicine, Radboud University Medical Center, Nijmegen, The Netherlands
5Center for Proteomics and Metabolomics, Leiden University Medical Center, Leiden, The Netherlands
6Department of Human Genetics, Leiden University Medical Center, Leiden, The Netherlands
7Department of Hematology, Leiden University Medical Center, Leiden, The Netherlands

Corresponding author:
Erik B. van den Akker, PhD; Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, The Netherlands; Einthovenweg 20, 2333 ZC, Leiden, The Netherlands; Tel: +31 (0)71 526 85 57; Fax: +31 (0)71 526 82 80; E-mail: e.b.van_den_akker@lumc.nl
Introduction
Acute myeloid leukaemia (AML) patients are currently subtyped based on recurrent genetic abnormalities (RGAs), with the most current classification system being the International Consensus Classification (ICC) 2022. The identification of genetic abnormalities relies on a range of methods, including cytogenetics, fluorescence in situ hybridisation, and targeted genetic analysis. While these genomic evaluation methods are accurate, they require specialised expertise and can be hindered by hard-to-sequence DNA regions or rare RGAs that escape standard laboratory procedures.
Gene expression profiling offers an alternative approach for AML evaluation by providing a transcriptomic-based snapshot of leukaemic samples. Previous research has demonstrated that genetic abnormalities drive specific gene expression signatures in AML patients, and that AML subtypes can be predicted from gene expression data. However, these earlier works do not comprehensively cover the ICC 2022 classification and critically lack rigorous evaluation of cross-cohort generalisability. This gap is significant: differences in sequencing platforms, sample processing, and patient populations between laboratories can severely compromise model performance and prevent clinical translation. 
Here we developed a machine learning ensemble classifier that predicts 16 ICC 2022 AML subtypes from gene expression data across seven independent cohorts (n=2,246) encompassing adult and paediatric patients. Through systematic nested cross-cohort validation and external testing on an independent cohort (n=685), we demonstrate robust classification performance and accurate cross-cohort predictions without batch effect correction. By implementing a reject option for low-confidence predictions, we achieve a Cohen's kappa of xxx whilst flagging only xx% of samples for manual review. We further demonstrate that [...], providing a valuable complementary diagnostic tool for AML subtyping.


Methods

Transcriptional data
We acquired RNA-seq data from primary AML samples derived from peripheral blood or bone marrow across seven independent cohorts for model development and performance validation: BEATAML1.0-COHORT, TCGA-LAML, LEUCEGENE, AAML0531, AAML1031, AAML03P1, and 100LUMC (in-house). For external validation, we obtained an independent dataset from the Munich Leukaemia Laboratory (MLL). Study-specific data availability statements and sequencing protocols are reported in the original publications.
Quantified gene counts for BEATAML1.0-COHORT, TCGA-LAML, and the three AAML cohorts (TARGET paediatric studies) were obtained from the Genomic Data Commons portal (https://portal.gdc.cancer.gov, release 36). For LEUCEGENE and 100LUMC, we used gene counts as described before [ref] where FASTQ files were processed using an identical bioinformatics pipeline to harmonise quantification: reads were aligned using STAR (v2.7.x) to the GRCh38 reference genome and gene-level counts were summarised using GENCODE v36 annotation (60,600 genes). The MLL cohort (n=685) was obtained on request and processed using the same STAR/GENCODE pipeline to ensure compatibility.

AML subtype annotation
For the seven training cohorts (BEATAML1.0-COHORT, TCGA-LAML, LEUCEGENE, AAML0531, AAML1031, AAML03P1 and 100LUMC), we harmonised genetic annotations (gene symbols, fusion transcripts, and karyotypes) and annotated samples according to the ICC 2022 classification system. Samples with comprehensive genetic characterisation but no identified RGA were annotated as "No RGA found". Samples with incomplete genetic data or indeterminate findings were classified as "Inconclusive". Samples with multiple RGAs without a defined leading alteration according to ICC 2022 hierarchy were classified as "Multiple". For model training and evaluation, we excluded samples classified as "Inconclusive" or "Multiple", additionally we excluded subtypes with less than 10 samples per subtype. 
For the external validation MLL cohort, we directly used the clinical genetic annotations provided by the laboratory as ground truth labels, following their established diagnostic workflow.
Model training and evaluation
All models were trained using the seven cohorts (BEATAML1.0-COHORT, TCGA-LAML, LEUCEGENE, AAML0531, AAML1031, AAML03P1 and 100LUMC). We evaluated three machine learning algorithms for AML subtype prediction: Support Vector Machines (SVM) with radial basis function kernel, XGBoost (gradient boosting), and deep neural networks (DNN).
Model performance was assessed using nested cross-validation under two complementary regimes: (1) stratified 5x5-fold cross-validation with random splits, and (2) leave-one-study-out (LOSO) cross-validation, where each of the seven training cohorts served sequentially as the held-out test set. Hyperparameters were optimised on inner folds, and final model performance was evaluated on outer folds using Cohen's kappa statistic as the evaluation metric to account for class imbalance. The best model was selected based on the highest Cohen’s kappa score.
For SVM, we tuned the regularisation parameter C and kernel coefficient γ. For XGBoost, we optimised tree depth, learning rate, number of estimators, and regularisation parameters. Class imbalance was addressed through optional class weighting for all three algorithms. For DNNs, we trained fully connected architectures with 2-3 hidden layers (architectures ranging from [200,100,50] to [800,400,200] neurons), ReLU activation, dropout (0.5), L2 regularisation, and Adam optimiser. Networks were trained using focal loss (γ=2) with early stopping on validation loss to minimise the number of epochs run. 
Following cross-validation, we trained our final framework on all seven training cohorts combined and performed final model performance evaluation on the completely held-out MLL cohort. The same preprocessing pipeline, normalisation procedures, and probability cutoffs optimised during cross-validation were applied to the MLL data without any cohort-specific adjustments.
Ensemble learning
Ensemble weights were optimised on inner cross-validation folds and applied to outer test folds. We generated weight combinations (SVM, XGBoost, DNN) ranging from 0 to 1 in 0.05 increments, normalised to sum to 1, yielding 231 unique configurations. For each outer fold, the optimal weight combination was selected based on maximum Cohen's kappa across inner folds.
We implemented two ensemble strategies: (1) Global ensemble: a single set of weights applied uniformly across all classes; and (2) One-versus-rest (OvR) ensemble: class-specific weights optimised independently for each AML subtype. For each prediction, class probabilities from individual models were weighted and the class with the highest weighted probability was selected.
Data preprocessing and normalisation
Within each cross-validation fold, raw gene counts were normalised using DESeq2-style size factors (geometric mean of ratios), log₂-transformed, and Z-score standardised. All normalisation parameters (size factors, means, and standard deviations) were estimated from training data and applied to test data. For SVM and XGBoost, we employed a one-versus-rest (OvR) multiclass strategy. For DNNs, we used a unified softmax output layer. Final class predictions were determined by the class with the highest predicted probability.
Variable gene selection
To promote cross-cohort generalisability and mitigate technical batch effects, we developed a consensus-based feature selection strategy that identifies genes with consistently high variability across all cohorts. Within each training fold, genes were ranked independently per cohort according to their median absolute deviation (MAD) from median expression. The top n genes from each cohort were selected, and only genes appearing in all cohort-specific lists (intersection) were retained as features. Feature selection was performed within each cross-validation fold using only training data. The number of top genes (n) was treated as a hyperparameter and optimised during model selection.
Prediction confidence cutoffs
For each model, probability cutoffs ranging from 0.00 to 1.00 (in 0.01 increments) were evaluated on inner cross-validation folds. Samples with maximum predicted probability below the threshold were flagged as low-confidence predictions. Optimal cutoffs were selected per outer fold by maximising Cohen's kappa whilst constraining: (1) less than 5% of total samples rejected, and (2) accuracy on rejected samples below 50%. Cutoff optimisation was performed independently for both cross-validation regimes and the individual and ensemble models.
Implementation
Machine learning implementations were performed in Python using scikit-learn, XGBoost, and TensorFlow with Keras. The focal loss implementation for the DNN was obtained from the focal-loss package. Models were trained on a high-performance computing cluster with parallelisation across hyperparameter configurations. Statistical analysis, data analysis and visualisation were performed using R and R-studio.
Results
Training dataset characteristics
After applying inclusion criteria, our primary dataset comprised 2,246 AML samples across seven independent cohorts, representing 16 ICC 2022-defined subtypes (Table 1). The dataset included both adult cohorts (BEATAML1.0-COHORT, TCGA-LAML, LEUCEGENE, 100LUMC) and paediatric cohorts (AAML0531, AAML1031, AAML03P1), providing diverse patient populations and technical platforms for cross-cohort evaluation. Subtype prevalence varied considerably, reflecting the natural heterogeneity of AML: more common subtypes such as NPM1-mutated (20.2%, n=454) and RUNX1::RUNX1T1 (13.1%, n=295) were well represented, whilst we also included rare subtypes such as FUS::ERG (0.4%, n=10) and KAT6A::CREBBP (0.5%, n=12) to enable the detection of very rare entities. 
Overall model performance
We evaluated five classification approaches: three individual models (SVM, XGBoost, DNN) and two ensemble strategies (Global ensemble and OvR ensemble) to predict ICC 2022 AML subtypes from gene expression alone. To quantify prediction performance, we implemented two complementary nested cross-validation regimes: 1) standard nested 5×5-fold cross-validation to assess overall performance, and 2) nested leave-one-study-out (LOSO) cross-validation where we sequentially held out each of the seven cohorts to evaluate generalisability to unseen datasets. In both regimes, we optimised hyperparameters in the inner folds and evaluated final performance on independent outer folds, ensuring unbiased performance estimates.
The Global ensemble, which combines predictions from all three individual models with optimised weights, consistently outperformed individual models in both validation regimes (Figure 1). Under standard nested cross-validation, the Global ensemble achieved a mean Cohen's kappa of 0.924 (SD: 0.012) and accuracy of 93.5% (SD: 1.0%) across the five outer folds. Under the more stringent LOSO regime the Global ensemble maintained strong performance with a mean Cohen's kappa of 0.911 (SD: 0.042) and accuracy of 92.7% (SD: 3.8%) across the seven outer folds. Notably, the ensemble strategy proved particularly valuable in the LOSO setting, where it demonstrated a larger performance advantage over individual models than in standard cross-validation, showing that the ensemble method aids in robust cross-cohort predictions.
Per-class performance evaluation
We next examined how classification performance varied across the 16 AML subtypes (Figure X). The Global ensemble achieved excellent F1-scores for well-represented subtypes including for example RUNX1::RUNX1T1 (F1=0.99 CV, 0.99 LOSO) and NPM1-mutated AML (F1=0.99 CV, 0.99 LOSO), demonstrating robust classification across both validation regimes. Notably, several rare subtypes also achieved strong performance, including KAT6A::CREBBP (F1=0.90 CV, 0.93 LOSO) and FUS::ERG (F1=0.87 CV, 0.91 LOSO), indicating that our framework captures distinctive transcriptional signatures even for uncommon entities. Conversely, some rare subtypes showed lower F1-scores in cross-validation, and three rare subtypes (RBM15::MRTF1, NUP98::KDM5A, CBFA2T3::GLIS2) could not be evaluated in LOSO validation due to their limited distribution across cohorts. 
KMT2A-related subtypes proved particularly challenging in the LOSO setting, with MLLT3::KMT2A (F1=0.89 CV, 0.79 LOSO) and Other KMT2A rearrangements (F1=0.83 CV, 0.73 LOSO) showing substantial performance drops. The confusion matrices revealed that KMT2A fusion subtypes were predominantly confused with each other, with 20 MLLT3::KMT2A samples misclassified as Other KMT2A rearrangements in LOSO validation. Additionally, MDS-related and MECOM rearrangement (F1=0.89 CV, 0.84 LOSO) emerged as a consistently challenging class, showing widespread misclassifications across multiple subtypes in both validation regimes. These findings demonstrate that our framework achieves reliable classification for most subtypes whilst certain molecular entities with variable representation remain more challenging to predict consistently.
Per fold and cohort performance analysis
Next, we analysed the performance over cohort in the LOSO setting, to evaluate the performance of our model over different patient cohorts (Figure X). As expected, in the LOSO setting performance varied more than in the standard CV setting, given the stratified distribution of classes in the standard CV setting. When comparing three paediatric cohorts (AAML0531, AAML1031, AAML03P1) to the four adult cohorts we found lower mean performance (kappa: 0.88±0.03) compared to adult cohorts (kappa: 0.94±0.03) (difference kappa: 0.05). We found that, in the LOSO context, classes that were enriched in the adult cohort showed higher F1 performance scores in both the adult and pediatric cohorts. But, we also found that pediatric-enriched classes also had on average a higher performance score in the adult cohorts. Showing that pediatric-enriched classes are harder to predict and predictions are in general harder for pediatric patients.
Examination of Global Ensembl weights
Examination of a reject option
We further examined whether incorporating a probability threshold as a reject option could improve classification performance. We optimised probability cutoffs within the inner folds of both nested cross-validation regimes, allowing us to flag low-confidence predictions. Implementing this reject option increased the mean Cohen's kappa to 0.939 in standard cross-validation (improvement: +0.015) and to 0.923 in LOSO validation (improvement: +0.012), whilst flagging 2.1% and 2.3% of samples, respectively, as low-confidence predictions. This demonstrates that our framework can identify uncertain predictions, further enhancing its practical utility.
Examination of selected features
Examination on validation dataset
Finally, we trained our final ensemble on all 2,246 samples from the primary dataset and evaluated its performance on the MLL validation cohort. The independent Munich Leukaemia Laboratory dataset comprised 659 samples after filtering, representing predominantly adult AML subtypes (Table 2). This cohort was annotated independently by the Munich laboratory using their established clinical diagnostic workflow, providing a stringent test of cross-laboratory performance. The ensemble achieved a Cohen's kappa of 0.940 and accuracy of 95.4% on this external cohort. When applying the rejection method, performance further improved to a kappa of 0.950 (improvement: +0.010) and accuracy of 96.2% (improvement: +0.8%), whilst flagging 1.4% of samples as low-confidence predictions. These results confirm that our framework generalises effectively to completely independent cohorts with different sequencing protocols and clinical annotation procedures, demonstrating its potential for real-world clinical application.


